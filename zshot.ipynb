{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 11:29:43.116593: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-14 11:29:43.302001: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741931983.386669    3348 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741931983.404539    3348 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-14 11:29:43.543055: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_port = pd.read_csv(\"Sentence pairs in English-Portuguese - 2025-02-23.tsv\",delimiter=\"\\t\",quoting=3,escapechar=\"/\",header=None)\n",
    "eng_port.columns = ['en_no',\"en_sent\",\"port_no\",\"port_sent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_span = pd.read_csv(\"Sentence pairs in Spanish-English - 2025-02-22.tsv\",delimiter=\"\\t\",quoting=3,escapechar=\"/\",header=None)\n",
    "eng_span.columns = ['span_no',\"span_sent\",\"en_no\",\"en_sent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_no</th>\n",
       "      <th>en_sent</th>\n",
       "      <th>port_no</th>\n",
       "      <th>port_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1276</td>\n",
       "      <td>Let's try something.</td>\n",
       "      <td>180624</td>\n",
       "      <td>Vamos tentar alguma coisa!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1276</td>\n",
       "      <td>Let's try something.</td>\n",
       "      <td>419917</td>\n",
       "      <td>Vamos tentar algo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1277</td>\n",
       "      <td>I have to go to sleep.</td>\n",
       "      <td>182184</td>\n",
       "      <td>Preciso ir dormir.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1277</td>\n",
       "      <td>I have to go to sleep.</td>\n",
       "      <td>893320</td>\n",
       "      <td>Tenho que ir dormir.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1277</td>\n",
       "      <td>I have to go to sleep.</td>\n",
       "      <td>1502853</td>\n",
       "      <td>Preciso dormir.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   en_no                 en_sent  port_no                   port_sent\n",
       "0   1276    Let's try something.   180624  Vamos tentar alguma coisa!\n",
       "1   1276    Let's try something.   419917          Vamos tentar algo!\n",
       "2   1277  I have to go to sleep.   182184          Preciso ir dormir.\n",
       "3   1277  I have to go to sleep.   893320        Tenho que ir dormir.\n",
       "4   1277  I have to go to sleep.  1502853             Preciso dormir."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_port.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span_no</th>\n",
       "      <th>span_sent</th>\n",
       "      <th>en_no</th>\n",
       "      <th>en_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2481</td>\n",
       "      <td>¡Intentemos algo!</td>\n",
       "      <td>1276</td>\n",
       "      <td>Let's try something.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2482</td>\n",
       "      <td>Tengo que irme a dormir.</td>\n",
       "      <td>1277</td>\n",
       "      <td>I have to go to sleep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2483</td>\n",
       "      <td>¿Qué estás haciendo?</td>\n",
       "      <td>16492</td>\n",
       "      <td>What are you doing?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2483</td>\n",
       "      <td>¿Qué estás haciendo?</td>\n",
       "      <td>241947</td>\n",
       "      <td>What are you doing now?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2483</td>\n",
       "      <td>¿Qué estás haciendo?</td>\n",
       "      <td>516639</td>\n",
       "      <td>What are you up to?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   span_no                 span_sent   en_no                  en_sent\n",
       "0     2481         ¡Intentemos algo!    1276     Let's try something.\n",
       "1     2482  Tengo que irme a dormir.    1277   I have to go to sleep.\n",
       "2     2483      ¿Qué estás haciendo?   16492      What are you doing?\n",
       "3     2483      ¿Qué estás haciendo?  241947  What are you doing now?\n",
       "4     2483      ¿Qué estás haciendo?  516639      What are you up to?"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_span.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of eng_span:270677\n",
      "len of eng_port:294789\n"
     ]
    }
   ],
   "source": [
    "print(f\"len of eng_span:{len(eng_span)}\")\n",
    "print(f\"len of eng_port:{len(eng_port)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using minimum dataset size: 270677\n"
     ]
    }
   ],
   "source": [
    "min_size = min(len(eng_span), len(eng_port))\n",
    "print(\"Using minimum dataset size:\", min_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Length: 270677 270677\n"
     ]
    }
   ],
   "source": [
    "eng_span = eng_span.sample(n=min_size, random_state=42).reset_index(drop=True)\n",
    "eng_port = eng_port.sample(n=min_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"New Length:\", len(eng_span), len(eng_port))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in /home/admin/Desktop/deep_learning/.venv/lib/python3.11/site-packages (0.9.3)\n",
      "Requirement already satisfied: pybind11>=2.2 in /home/admin/Desktop/deep_learning/.venv/lib/python3.11/site-packages (from fasttext) (2.13.6)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /home/admin/Desktop/deep_learning/.venv/lib/python3.11/site-packages (from fasttext) (66.1.1)\n",
      "Requirement already satisfied: numpy in /home/admin/Desktop/deep_learning/.venv/lib/python3.11/site-packages (from fasttext) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3642935171.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    gunzip cc.en.300.vec.gz\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "gunzip cc.en.300.vec.gz\n",
    "gunzip cc.es.300.vec.gz\n",
    "gunzip cc.pt.300.vec.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "import numpy as np\n",
    "\n",
    "# Load English FastText model\n",
    "# fasttext.util.download_model('en', if_exists='ignore')  # Download if not done already\n",
    "ft_en = fasttext.load_model('cc.en.300.vec')\n",
    "\n",
    "# Load Spanish FastText model\n",
    "# fasttext.util.download_model('es', if_exists='ignore')\n",
    "# ft_es = fasttext.load_model('cc.es.300.bin')\n",
    "\n",
    "# Load Portuguese FastText model\n",
    "fasttext.util.download_model('pt', if_exists='ignore')\n",
    "ft_pt = fasttext.load_model('cc.pt.300.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_vec_file(file):\n",
    "# Initialize an empty dictionary to hold the embeddings\n",
    "    embeddings = {}\n",
    "\n",
    "    # Open the .vec file\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        # Read the first line to get the number of vectors and their dimensions\n",
    "        first_line = f.readline().strip()\n",
    "        vocab_size, vector_dim = map(int, first_line.split())\n",
    "\n",
    "        # Iterate over each subsequent line\n",
    "        for line in f:\n",
    "            # Split the line into the word and its vector components\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=np.float16)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "# Access a word vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_en = read_vec_file(\"cc.en.300.vec\")\n",
    "vector = ft_en.get('example')  # Replace 'example' with your word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_pt = read_vec_file(\"cc.pt.300.vec\")\n",
    "vector = ft_pt.get('example')  # Replace 'example' with your word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_es = read_vec_file(\"cc.es.300.vec\")\n",
    "vector = ft_es.get('example')  # Replace 'example' with your word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.020e-02,  1.700e-03, -3.391e-02,  1.000e-04, -3.391e-02,\n",
       "       -3.531e-02,  5.450e-02, -2.150e-02,  1.570e-02, -5.501e-03,\n",
       "       -9.804e-03,  7.000e-03,  1.340e-02,  4.050e-02, -5.771e-02,\n",
       "        2.670e-02,  4.291e-02,  1.730e-02,  5.139e-02,  4.160e-02,\n",
       "       -3.500e-03, -4.401e-02,  4.550e-02, -4.611e-02, -6.830e-02,\n",
       "       -1.110e-02,  1.330e-02,  2.150e-02,  8.202e-03, -5.798e-03,\n",
       "        1.620e-02,  6.500e-03,  7.198e-03, -5.481e-02, -1.130e-02,\n",
       "       -9.399e-03,  4.001e-02, -5.521e-02, -0.000e+00, -5.191e-02,\n",
       "       -3.149e-02, -4.101e-03, -5.411e-02, -1.990e-02, -8.301e-03,\n",
       "        4.199e-02,  2.260e-02, -1.240e-02,  1.770e-02,  2.660e-02,\n",
       "        2.010e-02,  1.420e-02, -4.950e-02,  4.001e-04,  1.620e-02,\n",
       "       -3.241e-02, -5.731e-02, -1.440e-02, -1.190e-02, -3.180e-02,\n",
       "       -6.100e-03, -1.759e-02, -1.891e-02,  6.580e-02, -1.700e-02,\n",
       "       -2.390e-02,  1.110e-02, -1.470e-02, -3.180e-02, -1.360e-02,\n",
       "        9.903e-03,  1.961e-02,  8.998e-04,  2.850e-02,  1.200e-03,\n",
       "       -7.500e-03,  1.820e-02,  1.790e-02,  2.200e-02,  1.430e-02,\n",
       "       -1.280e-02,  1.680e-02, -5.261e-02,  6.049e-02, -3.081e-02,\n",
       "        2.199e-03,  4.510e-02, -2.260e-02,  8.797e-03,  5.920e-02,\n",
       "       -1.500e-02,  1.420e-02,  5.389e-02, -4.002e-03, -3.961e-02,\n",
       "        3.099e-03,  5.899e-02,  3.201e-03,  1.430e-02, -6.802e-03,\n",
       "       -2.451e-02,  3.401e-03,  4.599e-02, -1.400e-03,  2.049e-02,\n",
       "        1.630e-02, -8.400e-03, -3.220e-02, -4.181e-02, -4.001e-04,\n",
       "        2.280e-02,  1.050e-01, -6.901e-03,  2.299e-02, -1.669e-02,\n",
       "       -3.101e-02, -2.530e-02, -3.300e-03,  1.300e-03,  1.550e-02,\n",
       "       -3.369e-02, -3.599e-03,  8.698e-03,  1.759e-02, -1.240e-02,\n",
       "       -2.361e-02, -1.891e-02,  2.840e-02, -2.690e-02,  2.521e-02,\n",
       "        9.003e-03,  1.000e-03,  8.698e-03,  3.650e-02,  6.599e-03,\n",
       "        1.610e-02, -4.489e-02, -2.199e-03,  2.229e-02,  6.401e-03,\n",
       "        9.882e-02,  2.150e-02,  3.470e-02,  3.540e-02,  4.501e-03,\n",
       "        2.260e-02, -1.135e-01,  4.398e-03,  4.601e-03, -2.150e-02,\n",
       "       -2.139e-02,  1.649e-02,  1.700e-03,  1.820e-02, -2.400e-02,\n",
       "       -5.699e-03,  8.301e-02, -2.380e-02,  4.761e-02,  3.430e-02,\n",
       "       -5.679e-02,  2.870e-02, -1.840e-02, -1.649e-02,  1.891e-02,\n",
       "       -6.500e-03, -1.300e-02,  8.499e-03,  9.804e-03,  8.598e-03,\n",
       "       -5.402e-03, -2.040e-02,  7.013e-02, -3.549e-02, -3.901e-03,\n",
       "        1.000e-04, -5.901e-03, -9.499e-03, -2.049e-02, -2.760e-02,\n",
       "        8.598e-03,  7.330e-02, -1.360e-02,  7.361e-02, -4.700e-03,\n",
       "        4.849e-02, -4.902e-03, -1.320e-02,  3.619e-02,  2.180e-02,\n",
       "       -6.802e-03, -2.760e-02,  2.811e-02,  2.229e-02,  3.430e-02,\n",
       "        7.500e-03,  1.630e-02, -1.649e-02, -1.750e-02,  1.074e-01,\n",
       "       -2.190e-02,  2.010e-02, -1.740e-02,  8.698e-03, -9.300e-03,\n",
       "       -1.110e-02,  7.867e-02, -4.398e-03,  9.697e-03,  4.941e-02,\n",
       "       -1.010e-02,  1.300e-02,  1.048e-01,  1.790e-02,  1.100e-03,\n",
       "        1.000e-04,  2.040e-02,  4.190e-02,  5.199e-03,  6.149e-02,\n",
       "        1.500e-02,  3.799e-03,  2.800e-02,  3.860e-02, -1.060e-02,\n",
       "       -4.449e-02,  4.819e-02, -1.210e-02,  2.271e-02,  3.901e-03,\n",
       "        6.901e-03,  3.699e-02,  4.050e-02,  2.400e-02, -1.560e-02,\n",
       "        4.700e-03,  1.150e-02, -2.480e-02,  2.460e-02,  2.580e-02,\n",
       "       -2.330e-02,  7.000e-03, -5.688e-02, -2.260e-02,  9.387e-02,\n",
       "        1.691e-02, -5.699e-03, -2.240e-02,  2.580e-02, -4.001e-04,\n",
       "        5.600e-03,  3.500e-03,  5.750e-02, -4.340e-02,  2.451e-02,\n",
       "        1.871e-02, -2.890e-02, -4.599e-02,  2.820e-02, -1.570e-02,\n",
       "        9.102e-03,  4.901e-02, -1.830e-02, -1.640e-02, -7.928e-02,\n",
       "        3.641e-02,  3.729e-02,  1.000e-03,  4.709e-02, -4.059e-02,\n",
       "        8.998e-04, -3.909e-02,  2.361e-02, -4.019e-02, -7.471e-02,\n",
       "       -2.760e-02, -7.198e-03, -3.290e-02, -4.299e-03,  2.090e-02,\n",
       "       -5.199e-03,  1.660e-02, -6.250e-02, -1.050e-02, -1.360e-02,\n",
       "        2.300e-03, -1.570e-02, -8.202e-03,  1.250e-02, -4.980e-02,\n",
       "        6.708e-02,  2.409e-02, -4.902e-03,  1.270e-02, -2.130e-02,\n",
       "       -2.280e-02, -8.598e-03,  2.780e-02,  3.500e-03,  2.850e-02],\n",
       "      dtype=float16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Tokenizer function (simple word split)\n",
    "def tokenize(sentence):\n",
    "    return re.findall(r'\\b\\w+\\b', sentence.lower())  # Lowercase and split words\n",
    "\n",
    "\n",
    "# Convert sentence to FastText embedding\n",
    "def sentence_to_vector(sentence, ft_model, default_vector=None):\n",
    "    words = tokenize(sentence)\n",
    "    max_len = 20\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            # Try to get the embedding for the word\n",
    "            vectors.append(ft_model[word])\n",
    "        except KeyError:\n",
    "            # Handle OOV: use default vector or skip\n",
    "            vectors.append(default_vector)\n",
    "    if len(vectors) < max_len:\n",
    "        vectors.extend(np.zeros((max_len - len(vectors), 300)))\n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = sentence_to_vector(\"hello is it me your looking for\",ft_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 300)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1.57592773e-01,  4.37927246e-02, -4.50134277e-03,  6.65893555e-02,\n",
       "         7.70263672e-02,  4.90188599e-03,  8.20159912e-03,  6.50024414e-03,\n",
       "         9.30023193e-03,  3.54003906e-02, -2.31018066e-02, -4.91943359e-02,\n",
       "        -8.33129883e-02,  1.56021118e-02,  2.54882812e-01,  3.44848633e-02,\n",
       "        -1.06964111e-02, -7.80029297e-02, -7.08007812e-02,  7.61718750e-02,\n",
       "        -6.10046387e-02,  4.48913574e-02, -7.29980469e-02,  1.30996704e-02,\n",
       "         3.14941406e-02, -3.10058594e-02,  1.66015625e-02,  1.73950195e-02,\n",
       "        -7.36083984e-02,  1.18225098e-01, -1.21276855e-01, -4.08935547e-02,\n",
       "         2.94036865e-02,  4.84008789e-02, -1.33972168e-02, -1.75018311e-02,\n",
       "         7.50732422e-02,  9.96704102e-02, -4.00085449e-02,  4.10079956e-03,\n",
       "        -7.22045898e-02, -4.43115234e-02, -1.19972229e-03,  7.56835938e-02,\n",
       "         3.97949219e-02,  3.22875977e-02,  1.96075439e-02,  4.68139648e-02,\n",
       "        -1.46026611e-02,  1.12976074e-01,  3.14941406e-02, -1.02294922e-01,\n",
       "         1.58081055e-01, -2.76031494e-02, -3.39965820e-02, -1.77001953e-02,\n",
       "        -5.99861145e-04,  1.10778809e-01, -1.64947510e-02, -3.09944153e-03,\n",
       "        -4.22973633e-02,  1.11389160e-01, -5.31005859e-02,  4.91027832e-02,\n",
       "         9.10034180e-02,  6.56738281e-02, -3.71093750e-02,  3.82080078e-02,\n",
       "         7.25097656e-02, -5.31921387e-02,  3.05938721e-02, -5.77087402e-02,\n",
       "        -8.06884766e-02, -9.05761719e-02, -8.05053711e-02, -6.03027344e-02,\n",
       "        -9.72900391e-02,  4.83093262e-02,  6.79931641e-02, -2.59971619e-03,\n",
       "        -8.59832764e-03, -5.10025024e-03,  3.15856934e-02,  6.67114258e-02,\n",
       "         2.99930573e-04, -8.34960938e-02,  4.44946289e-02,  3.60107422e-02,\n",
       "        -2.07061768e-02, -6.21032715e-02, -9.08203125e-02, -4.87976074e-02,\n",
       "         1.32812500e-01,  1.26037598e-02,  4.61120605e-02, -5.53894043e-02,\n",
       "         2.30026245e-03,  4.91943359e-02,  3.35998535e-02,  6.64062500e-02,\n",
       "        -8.92944336e-02, -5.37109375e-02,  1.32202148e-01, -9.10186768e-03,\n",
       "         3.29971313e-03, -4.37011719e-02,  7.51953125e-02, -4.37011719e-02,\n",
       "        -3.93066406e-02,  4.90112305e-02,  8.06274414e-02, -3.93981934e-02,\n",
       "        -7.59887695e-02,  7.17163086e-02, -1.89056396e-02, -4.21142578e-02,\n",
       "         3.29971313e-03, -2.13928223e-02, -1.30126953e-01,  1.37023926e-02,\n",
       "        -5.15136719e-02,  3.86962891e-02,  4.92858887e-02, -6.17980957e-02,\n",
       "        -3.39965820e-02,  3.51867676e-02,  2.58941650e-02, -1.02783203e-01,\n",
       "         6.00891113e-02, -7.14111328e-02, -2.23999023e-02, -1.03393555e-01,\n",
       "        -6.34765625e-02,  1.19972229e-03, -8.39996338e-03, -7.09838867e-02,\n",
       "        -1.39007568e-02,  9.30175781e-02, -7.61718750e-02, -1.80053711e-01,\n",
       "         4.98046875e-02,  5.59997559e-02,  4.37011719e-02,  1.69067383e-02,\n",
       "        -3.51867676e-02,  5.50079346e-03, -1.51733398e-01,  8.30078125e-03,\n",
       "         1.33911133e-01,  1.18408203e-01, -2.54974365e-02, -5.89904785e-02,\n",
       "        -1.15478516e-01, -9.11865234e-02, -3.25927734e-02,  9.59777832e-03,\n",
       "         7.08007812e-02, -1.19628906e-01, -2.45056152e-02,  4.66918945e-02,\n",
       "        -1.05773926e-01,  8.39996338e-03, -3.58886719e-02, -7.12280273e-02,\n",
       "         1.49047852e-01, -9.41162109e-02,  3.87878418e-02,  4.80041504e-02,\n",
       "         2.00042725e-02,  5.70068359e-02, -5.09033203e-02, -1.55029297e-02,\n",
       "        -3.21044922e-02,  6.40258789e-02,  4.45861816e-02, -5.41992188e-02,\n",
       "         2.38952637e-02,  3.98864746e-02,  4.94995117e-02, -8.12988281e-02,\n",
       "         8.67919922e-02,  2.78930664e-02,  2.22930908e-02,  6.87866211e-02,\n",
       "         5.80139160e-02,  1.23977661e-02,  9.17968750e-02,  1.69982910e-02,\n",
       "        -2.20947266e-02, -5.55114746e-02,  3.20053101e-03, -8.94775391e-02,\n",
       "        -5.99861145e-04, -4.80957031e-02, -4.11071777e-02, -3.46984863e-02,\n",
       "        -4.22973633e-02,  1.01074219e-01,  4.34875488e-02,  6.75048828e-02,\n",
       "        -7.33032227e-02,  2.33001709e-02,  3.76892090e-02,  9.00268555e-03,\n",
       "        -8.25195312e-02, -9.68017578e-02,  5.90133667e-03,  2.61993408e-02,\n",
       "        -2.22930908e-02,  7.39135742e-02, -1.89971924e-03, -9.77783203e-02,\n",
       "        -5.38024902e-02, -4.76989746e-02, -1.30004883e-02,  8.00132751e-04,\n",
       "         2.90069580e-02, -3.09944153e-03, -9.28955078e-02,  6.73828125e-02,\n",
       "        -1.85546875e-01,  4.01000977e-02, -5.63049316e-02,  6.18896484e-02,\n",
       "         8.94165039e-02, -6.90917969e-02, -3.21960449e-02, -1.35375977e-01,\n",
       "        -7.45849609e-02,  1.01501465e-01, -2.70080566e-03,  6.06994629e-02,\n",
       "         2.43072510e-02, -1.51855469e-01, -2.94036865e-02, -4.19998169e-03,\n",
       "         5.16052246e-02,  1.86035156e-01, -2.56042480e-02,  8.11767578e-02,\n",
       "         3.20053101e-03, -3.35998535e-02,  3.90014648e-02, -7.37915039e-02,\n",
       "         1.14624023e-01, -1.00016594e-04, -3.68957520e-02,  9.30786133e-02,\n",
       "        -2.92968750e-02,  5.20935059e-02,  8.00323486e-03, -2.92968750e-02,\n",
       "         1.31225586e-01, -8.31909180e-02, -3.39965820e-02,  1.21276855e-01,\n",
       "         3.50952148e-02,  4.19998169e-03,  5.02929688e-02,  2.05993652e-02,\n",
       "         7.89794922e-02, -4.94995117e-02,  2.54058838e-02, -2.96020508e-02,\n",
       "        -2.65045166e-02,  5.42907715e-02, -5.52978516e-02,  1.06964111e-02,\n",
       "        -2.99987793e-02, -6.04858398e-02,  8.53881836e-02, -6.65893555e-02,\n",
       "        -6.78100586e-02,  3.51867676e-02,  6.20117188e-02,  4.80957031e-02,\n",
       "        -3.44848633e-02, -2.87017822e-02, -5.91125488e-02, -5.10025024e-03,\n",
       "        -9.74121094e-02,  1.89971924e-03, -9.05761719e-02,  1.48010254e-02,\n",
       "        -9.77783203e-02,  3.96118164e-02,  2.83050537e-02, -9.27734375e-02,\n",
       "        -8.20159912e-03, -4.56848145e-02,  1.12304688e-01,  8.59985352e-02,\n",
       "        -1.47460938e-01,  8.33129883e-02,  9.94873047e-02, -3.67126465e-02,\n",
       "         6.84814453e-02,  8.06884766e-02, -4.50134277e-02, -3.10974121e-02]),\n",
       " array([-9.77783203e-02, -2.08251953e-01, -1.03698730e-01, -1.60064697e-02,\n",
       "        -2.40234375e-01, -4.48913574e-02,  3.00025940e-03,  9.89990234e-02,\n",
       "        -4.04052734e-02, -5.04150391e-02, -9.24072266e-02,  4.04052734e-02,\n",
       "         8.30078125e-02, -2.56042480e-02, -1.78833008e-01,  3.07861328e-01,\n",
       "         2.85034180e-02, -8.08105469e-02, -4.33044434e-02, -4.19921875e-02,\n",
       "        -1.58996582e-02,  1.14685059e-01, -5.51147461e-02, -3.39050293e-02,\n",
       "         1.65039062e-01, -2.00958252e-02,  1.49993896e-02,  6.26220703e-02,\n",
       "        -2.05566406e-01,  3.04687500e-01, -9.58251953e-02,  6.00051880e-03,\n",
       "        -6.51245117e-02,  3.64074707e-02, -1.57928467e-02, -1.82037354e-02,\n",
       "         2.08618164e-01, -4.44335938e-01,  1.90917969e-01, -1.08276367e-01,\n",
       "        -1.45141602e-01, -2.09045410e-02, -5.56030273e-02, -5.67016602e-02,\n",
       "         2.25952148e-01, -5.46875000e-02, -8.39233398e-02,  1.60766602e-01,\n",
       "         9.38720703e-02,  2.74963379e-02, -3.32946777e-02, -2.94921875e-01,\n",
       "        -8.20922852e-02, -9.39941406e-02,  4.29916382e-03, -1.22985840e-01,\n",
       "        -6.02111816e-02, -2.59765625e-01, -8.17260742e-02,  5.39855957e-02,\n",
       "         1.15966797e-02, -1.10717773e-01, -4.86145020e-02, -2.98095703e-01,\n",
       "        -2.63061523e-02,  1.60980225e-02, -5.85021973e-02, -3.79943848e-03,\n",
       "         3.79943848e-02,  3.68041992e-02,  1.95068359e-01, -9.36889648e-02,\n",
       "        -5.99861145e-04,  1.38015747e-02, -3.46984863e-02, -1.00891113e-01,\n",
       "         1.68579102e-01,  5.23986816e-02, -3.77197266e-01,  3.09944153e-03,\n",
       "         8.09936523e-02, -1.48010254e-02, -8.09936523e-02,  7.72094727e-02,\n",
       "        -2.65625000e-01, -1.27075195e-01,  2.90039062e-01, -1.38427734e-01,\n",
       "         1.15173340e-01, -2.14996338e-02, -7.14111328e-02, -1.16821289e-01,\n",
       "         1.92260742e-01, -1.19506836e-01, -7.92236328e-02,  5.57617188e-01,\n",
       "        -5.47363281e-01,  7.61718750e-02, -8.28857422e-02, -5.12084961e-02,\n",
       "        -2.00080872e-03,  8.61816406e-02,  1.13403320e-01, -2.49938965e-02,\n",
       "        -9.77172852e-02,  1.12670898e-01,  9.17968750e-02,  3.10058594e-02,\n",
       "         8.00323486e-03,  1.22497559e-01, -1.70040131e-03,  1.59179688e-01,\n",
       "         3.68957520e-02, -1.09802246e-01,  4.29992676e-02, -6.46972656e-02,\n",
       "         3.21044922e-02,  1.11999512e-01, -3.24096680e-02, -1.66259766e-01,\n",
       "         0.00000000e+00,  1.16882324e-01,  1.25366211e-01,  2.79998779e-03,\n",
       "        -1.11999512e-02, -5.78918457e-02, -7.28759766e-02, -5.16052246e-02,\n",
       "        -5.68847656e-02,  1.85791016e-01,  1.51977539e-02, -1.07177734e-01,\n",
       "         4.02587891e-01, -7.94067383e-02,  1.38015747e-02, -1.93939209e-02,\n",
       "        -5.64941406e-01, -1.10778809e-01,  1.08581543e-01,  1.31835938e-01,\n",
       "         3.54919434e-02,  3.40881348e-02, -1.21398926e-01,  6.29272461e-02,\n",
       "        -2.41821289e-01, -2.09999084e-03, -9.34082031e-01, -5.66101074e-02,\n",
       "        -2.38037109e-02, -1.14196777e-01, -3.44482422e-01, -3.79943848e-02,\n",
       "         1.13891602e-01,  7.45239258e-02,  2.22930908e-02,  5.68847656e-02,\n",
       "         2.18750000e-01, -5.31005859e-02, -6.81152344e-02,  2.25067139e-02,\n",
       "         5.27038574e-02, -6.90917969e-02,  1.21520996e-01,  2.52075195e-02,\n",
       "         5.45043945e-02, -1.11007690e-02, -2.29034424e-02, -1.49291992e-01,\n",
       "        -3.50036621e-02,  2.30026245e-03, -8.53881836e-02, -4.22058105e-02,\n",
       "         7.34863281e-02, -1.31988525e-02,  4.19998169e-03, -9.82055664e-02,\n",
       "        -4.79888916e-03,  7.91015625e-02, -1.05529785e-01,  4.83093262e-02,\n",
       "        -3.25012207e-02,  4.71435547e-01, -1.42089844e-01, -1.89056396e-02,\n",
       "         1.10717773e-01,  1.31225586e-01, -2.17651367e-01, -5.75866699e-02,\n",
       "         2.34069824e-02,  1.05285645e-01, -1.00097656e-01, -2.98583984e-01,\n",
       "        -7.67211914e-02,  4.15039062e-02,  1.01623535e-01,  4.14123535e-02,\n",
       "        -1.21002197e-02, -1.15905762e-01,  3.25012207e-02,  3.90052795e-03,\n",
       "        -4.76989746e-02,  2.93212891e-01,  3.64074707e-02,  4.19921875e-02,\n",
       "         1.52343750e-01,  7.61108398e-02,  1.78985596e-02, -3.49121094e-02,\n",
       "        -1.14379883e-01, -1.64947510e-02,  1.04980469e-02,  8.33129883e-02,\n",
       "        -3.96484375e-01,  1.68457031e-01, -3.87878418e-02, -7.18994141e-02,\n",
       "         1.87072754e-02, -1.30996704e-02,  6.71997070e-02,  1.56021118e-02,\n",
       "         1.75537109e-01, -5.02014160e-02, -1.39999390e-03,  1.85546875e-01,\n",
       "         8.80126953e-02, -1.46240234e-01,  1.67846680e-01,  6.51245117e-02,\n",
       "         8.86840820e-02,  3.20053101e-03,  3.50036621e-02,  5.99861145e-04,\n",
       "         3.06884766e-01, -6.46972656e-02, -4.69970703e-03, -2.83447266e-01,\n",
       "         2.49023438e-02, -1.19323730e-01, -1.07116699e-01,  1.66931152e-02,\n",
       "         2.74047852e-02, -1.80053711e-02,  2.38159180e-01,  8.49914551e-03,\n",
       "         1.33300781e+00, -1.60156250e-01,  8.97216797e-02,  1.16394043e-01,\n",
       "        -2.48413086e-01,  1.46606445e-01, -2.36968994e-02, -2.11944580e-02,\n",
       "        -8.53881836e-02, -1.99951172e-01, -1.34963989e-02, -5.26123047e-02,\n",
       "        -9.19189453e-02, -1.16088867e-01, -1.57226562e-01, -3.64074707e-02,\n",
       "        -1.78985596e-02,  6.76757812e-01, -1.02722168e-01, -9.13085938e-02,\n",
       "        -1.04675293e-01, -1.48193359e-01,  6.29882812e-02, -9.30175781e-02,\n",
       "         2.72705078e-01, -2.29858398e-01,  1.20010376e-02, -1.92260742e-01,\n",
       "         8.19702148e-02, -2.29034424e-02, -6.70166016e-02, -2.39379883e-01,\n",
       "        -4.95605469e-01, -1.64550781e-01, -7.59887695e-03, -2.12158203e-01,\n",
       "         8.06884766e-02,  9.38720703e-02, -7.33398438e-01, -2.53906250e-01,\n",
       "         7.09838867e-02, -2.94036865e-02,  1.80541992e-01, -7.25708008e-02,\n",
       "        -1.71966553e-02,  1.95007324e-02,  5.93872070e-02,  3.61938477e-02,\n",
       "        -1.82373047e-01,  1.40747070e-01, -8.67919922e-02,  2.43072510e-02,\n",
       "         6.59790039e-02, -2.28149414e-01, -2.24365234e-01,  8.89892578e-02]),\n",
       " array([ 3.70025635e-03, -2.41943359e-01, -8.97216797e-02, -8.36791992e-02,\n",
       "        -5.90133667e-03, -1.49993896e-02, -1.80053711e-01,  2.20031738e-02,\n",
       "        -3.28979492e-02, -7.56225586e-02,  3.83911133e-02, -3.93981934e-02,\n",
       "         2.43408203e-01, -7.70187378e-03, -1.02722168e-01,  3.09326172e-01,\n",
       "         8.17871094e-02, -9.47875977e-02, -6.06994629e-02,  2.47955322e-02,\n",
       "         1.77001953e-02,  4.90112305e-02, -1.12976074e-01, -1.63085938e-01,\n",
       "        -1.58569336e-01, -7.04956055e-02, -2.74047852e-02,  8.44726562e-02,\n",
       "        -1.45019531e-01,  4.36279297e-01,  1.71966553e-02,  3.49998474e-03,\n",
       "         4.76989746e-02, -8.03833008e-02,  2.29614258e-01, -3.64990234e-02,\n",
       "        -4.25109863e-02, -9.69238281e-02,  7.25708008e-02,  1.11007690e-02,\n",
       "        -8.30078125e-02, -7.92846680e-02,  1.70040131e-03,  2.16064453e-02,\n",
       "         1.38549805e-01,  8.47167969e-02,  1.50985718e-02,  2.82714844e-01,\n",
       "         2.99987793e-02, -1.98059082e-02,  5.01098633e-02, -7.84301758e-02,\n",
       "        -1.12182617e-01, -8.80126953e-02, -1.76269531e-01, -6.04858398e-02,\n",
       "        -7.51953125e-02,  1.74560547e-01, -1.47949219e-01, -1.43554688e-01,\n",
       "        -1.17797852e-01, -1.87072754e-02, -5.90133667e-03, -2.46948242e-01,\n",
       "        -3.98864746e-02,  1.19972229e-03, -5.99060059e-02, -8.41064453e-02,\n",
       "        -9.66186523e-02, -8.80126953e-02, -1.29516602e-01,  1.48010254e-02,\n",
       "         5.12084961e-02,  2.79998779e-03,  3.76892090e-02, -3.49121094e-02,\n",
       "         1.35742188e-01,  1.66992188e-01,  8.06274414e-02, -2.36968994e-02,\n",
       "         7.44018555e-02, -6.97021484e-02, -1.71966553e-02,  1.49414062e-01,\n",
       "        -1.17370605e-01, -9.66796875e-02,  2.55615234e-01, -1.14974976e-02,\n",
       "         1.36352539e-01, -1.80664062e-01,  1.10900879e-01, -1.19201660e-01,\n",
       "         3.15185547e-01, -7.72094727e-02,  1.67602539e-01,  2.52685547e-01,\n",
       "        -2.75878906e-01,  1.76757812e-01, -4.91943359e-02, -2.99987793e-02,\n",
       "         9.69848633e-02,  1.39648438e-01,  1.10595703e-01, -6.00891113e-02,\n",
       "        -7.40051270e-03, -3.14941406e-02,  7.31201172e-02,  7.14111328e-02,\n",
       "        -2.89916992e-03,  3.14941406e-02, -1.22985840e-02, -1.41845703e-01,\n",
       "        -5.40161133e-03,  6.20117188e-02, -7.67822266e-02, -1.28051758e-01,\n",
       "        -3.92150879e-02,  8.75244141e-02,  7.95288086e-02,  4.65087891e-02,\n",
       "        -3.93981934e-02,  2.72064209e-02,  9.21020508e-02,  1.10412598e-01,\n",
       "         1.73950195e-01,  1.42211914e-01, -4.45861816e-02, -2.50000000e-01,\n",
       "        -1.36718750e-01,  3.56140137e-02, -9.49859619e-03, -9.22851562e-02,\n",
       "         3.75000000e-01, -2.78808594e-01,  5.00106812e-03, -2.05322266e-01,\n",
       "        -2.89306641e-01, -3.17993164e-02,  9.74121094e-02,  6.56127930e-02,\n",
       "        -1.73339844e-01, -1.00891113e-01,  6.54907227e-02,  1.07299805e-01,\n",
       "        -2.50732422e-01,  1.79687500e-01, -9.78027344e-01,  2.07977295e-02,\n",
       "        -3.98864746e-02,  7.89642334e-03, -9.08813477e-02,  4.47998047e-02,\n",
       "        -5.09948730e-02,  2.46948242e-01,  4.21142578e-02, -5.09033203e-02,\n",
       "         1.83227539e-01,  1.57836914e-01,  8.30078125e-02,  8.31298828e-02,\n",
       "         3.08074951e-02, -5.00106812e-03,  1.66259766e-01,  4.76989746e-02,\n",
       "        -2.07061768e-02, -5.96923828e-02,  1.15722656e-01, -1.84814453e-01,\n",
       "        -1.69982910e-02,  1.03515625e-01, -4.19006348e-02,  3.35083008e-02,\n",
       "         5.29861450e-03, -1.12609863e-01,  3.50036621e-02, -1.89453125e-01,\n",
       "         5.70983887e-02, -3.80126953e-01,  8.28857422e-02,  4.11987305e-02,\n",
       "        -6.13098145e-02, -6.73217773e-02,  1.08093262e-01,  1.52221680e-01,\n",
       "         7.39135742e-02,  2.78808594e-01, -6.45751953e-02, -1.40258789e-01,\n",
       "        -5.81970215e-02, -9.49096680e-02, -1.36596680e-01, -6.45751953e-02,\n",
       "        -4.25109863e-02, -1.28417969e-01,  5.34973145e-02,  2.85034180e-02,\n",
       "        -8.72802734e-02, -7.00073242e-02,  1.03210449e-01, -3.68957520e-02,\n",
       "        -1.04003906e-01,  3.55712891e-01,  9.96093750e-02, -1.40747070e-01,\n",
       "         1.34887695e-01,  4.69055176e-02, -3.96240234e-01, -8.69140625e-02,\n",
       "        -4.04968262e-02, -2.29125977e-01, -1.21276855e-01,  9.02709961e-02,\n",
       "        -3.10302734e-01,  1.14929199e-01, -1.05972290e-02, -1.23291016e-01,\n",
       "         1.93115234e-01,  9.41162109e-02,  1.96166992e-01,  2.27050781e-02,\n",
       "         1.34765625e-01,  5.29861450e-03, -1.89056396e-02, -6.34765625e-02,\n",
       "         6.10961914e-02, -2.07641602e-01,  6.06079102e-02,  4.74853516e-02,\n",
       "        -2.29980469e-01, -3.17993164e-02,  5.24902344e-02, -6.17980957e-02,\n",
       "        -1.45507812e-01, -1.46240234e-01,  1.00016594e-04, -3.15917969e-01,\n",
       "        -2.45056152e-02,  6.98242188e-02,  6.81762695e-02,  0.00000000e+00,\n",
       "         1.50985718e-02,  1.07177734e-01,  9.82055664e-02, -1.86157227e-01,\n",
       "         7.38281250e-01, -6.26831055e-02, -1.34399414e-01,  2.57324219e-01,\n",
       "         3.05023193e-02,  2.05932617e-01,  1.22192383e-01, -2.63977051e-02,\n",
       "        -6.48803711e-02, -5.49011230e-02,  2.54058838e-02, -2.06787109e-01,\n",
       "         1.43966675e-02, -7.51953125e-02,  1.86157227e-01,  2.00042725e-02,\n",
       "         1.22985840e-02,  2.17895508e-01, -9.49707031e-02,  2.18963623e-02,\n",
       "         2.51007080e-02, -1.55639648e-01,  1.02416992e-01, -4.73022461e-02,\n",
       "         8.31909180e-02, -3.79882812e-01, -7.25708008e-02, -1.48315430e-01,\n",
       "         6.06994629e-02, -1.98120117e-01,  1.22009277e-01,  1.15112305e-01,\n",
       "        -2.80273438e-01, -2.29858398e-01,  7.09915161e-03, -3.93066406e-02,\n",
       "         9.80377197e-03,  2.11059570e-01, -5.34667969e-01, -1.02783203e-01,\n",
       "         2.30957031e-01, -1.12182617e-01,  4.01000977e-02, -1.99951172e-01,\n",
       "        -1.66870117e-01,  4.60052490e-03,  6.78100586e-02, -8.00781250e-02,\n",
       "        -5.36621094e-01, -1.62963867e-01, -1.23779297e-01,  1.95007324e-02,\n",
       "         4.19006348e-02,  1.05859375e+00, -2.16979980e-02, -1.05895996e-01]),\n",
       " array([ 1.82128906e-01, -4.17236328e-01,  1.79687500e-01, -7.75756836e-02,\n",
       "        -5.53894043e-02, -1.04980469e-01, -2.26928711e-01, -1.69067383e-02,\n",
       "        -4.32128906e-02,  3.24096680e-02,  3.57055664e-02, -1.36840820e-01,\n",
       "        -6.48193359e-02, -1.92016602e-01, -5.81054688e-02,  6.59790039e-02,\n",
       "        -7.37304688e-02,  4.30908203e-02, -1.85058594e-01,  2.01538086e-01,\n",
       "         2.38647461e-01,  1.76757812e-01,  7.12280273e-02, -1.24389648e-01,\n",
       "        -4.66064453e-01, -6.70776367e-02, -1.79443359e-01,  4.08935547e-02,\n",
       "        -4.50134277e-03,  6.74804688e-01, -3.02978516e-01, -5.96008301e-02,\n",
       "        -6.26220703e-02,  1.37817383e-01,  1.43676758e-01, -1.02416992e-01,\n",
       "        -2.02880859e-01,  2.13256836e-01, -2.54058838e-02,  9.00268555e-03,\n",
       "        -1.09069824e-01,  1.59301758e-01, -1.54541016e-01,  5.21972656e-01,\n",
       "         2.83691406e-01,  9.68017578e-02, -1.14685059e-01,  9.49096680e-02,\n",
       "        -1.91894531e-01, -1.21582031e-01,  1.26708984e-01, -1.01074219e-01,\n",
       "        -2.48046875e-01, -4.22058105e-02, -2.44995117e-01, -3.22021484e-01,\n",
       "        -1.69311523e-01,  6.69860840e-03, -2.01782227e-01, -2.20947266e-02,\n",
       "        -1.44165039e-01, -6.69860840e-03, -9.35058594e-02,  2.19917297e-03,\n",
       "        -1.62597656e-01, -1.28051758e-01, -4.58984375e-02,  1.52587891e-01,\n",
       "         4.98962402e-02, -6.21948242e-02, -1.87866211e-01, -8.61816406e-02,\n",
       "        -1.81152344e-01, -2.25830078e-01, -1.29013062e-02, -2.07641602e-01,\n",
       "        -8.22143555e-02,  1.91955566e-02,  2.25341797e-01,  7.75146484e-02,\n",
       "        -2.65960693e-02, -6.04858398e-02, -8.49914551e-03,  1.12304688e-01,\n",
       "         4.69970703e-01, -5.23071289e-02,  4.55566406e-01,  6.63574219e-01,\n",
       "         1.86767578e-01, -2.27050781e-01, -9.52758789e-02,  5.92041016e-02,\n",
       "         9.05273438e-01, -1.83837891e-01, -1.97143555e-01,  2.05200195e-01,\n",
       "         1.86889648e-01,  2.27539062e-01,  1.35009766e-01,  8.41064453e-02,\n",
       "         2.51953125e-01,  6.56738281e-02,  7.86132812e-02, -3.57910156e-01,\n",
       "         1.60156250e-01,  9.79003906e-02,  4.29077148e-02,  8.83178711e-02,\n",
       "         8.00170898e-02,  7.03125000e-02,  1.00097656e-01,  1.08703613e-01,\n",
       "         1.39999390e-03,  5.72265625e-01,  1.93725586e-01, -6.19888306e-03,\n",
       "        -1.46240234e-01,  1.07971191e-01,  3.59916687e-03,  1.55029297e-02,\n",
       "        -8.95996094e-02,  1.27563477e-01,  3.54003906e-01,  2.00042725e-02,\n",
       "         6.20117188e-02,  3.88916016e-01,  1.96075439e-02, -2.19482422e-01,\n",
       "         6.18896484e-02,  2.36694336e-01,  2.85034180e-02, -1.17919922e-01,\n",
       "         1.36718750e-01,  1.42944336e-01,  1.35742188e-01, -2.49145508e-01,\n",
       "        -2.78320312e-01, -3.34716797e-01, -1.25122070e-01, -1.23107910e-01,\n",
       "        -1.11328125e-01,  3.98864746e-02,  1.79958344e-03,  1.84936523e-02,\n",
       "         3.03710938e-01, -2.41455078e-01, -1.05175781e+00, -1.01501465e-01,\n",
       "         3.68041992e-02,  2.15087891e-01, -3.41796875e-01, -2.22015381e-02,\n",
       "        -1.01379395e-01,  9.93041992e-02, -1.14974976e-02, -1.04980469e-02,\n",
       "         1.92260742e-01, -9.27124023e-02,  1.22985840e-02,  5.21850586e-02,\n",
       "         6.40106201e-03,  1.53564453e-01, -6.09970093e-03, -6.16149902e-02,\n",
       "         2.50053406e-03, -1.27319336e-01,  7.48901367e-02, -3.51074219e-01,\n",
       "         1.23779297e-01,  1.85058594e-01, -7.20214844e-02, -9.89990234e-02,\n",
       "        -7.97729492e-02,  1.76879883e-01,  5.34057617e-02, -1.95556641e-01,\n",
       "         1.11022949e-01, -2.19360352e-01,  2.19360352e-01,  1.37695312e-01,\n",
       "         1.91650391e-01,  2.26928711e-01, -1.55029297e-02,  1.07910156e-01,\n",
       "         4.45861816e-02,  5.40039062e-01,  1.06079102e-01,  1.52954102e-01,\n",
       "         2.54394531e-01, -1.60980225e-02,  5.02929688e-02,  7.95898438e-02,\n",
       "         1.02996826e-02, -2.66601562e-01, -3.68957520e-02, -1.92260742e-01,\n",
       "        -9.10034180e-02, -1.46240234e-01,  2.57568359e-01, -1.26037598e-02,\n",
       "        -5.24902344e-02,  3.57421875e-01,  4.30908203e-02, -6.06079102e-02,\n",
       "        -3.82995605e-02,  1.44165039e-01, -2.01049805e-01,  5.08117676e-02,\n",
       "         1.38549805e-01, -1.17034912e-02,  1.06628418e-01, -1.80969238e-02,\n",
       "        -7.17773438e-02, -1.50024414e-01, -1.92749023e-01, -2.17285156e-01,\n",
       "         2.25097656e-01,  2.12036133e-01,  6.14013672e-02, -1.74682617e-01,\n",
       "         9.16748047e-02,  4.90112305e-02, -1.80053711e-01,  5.00202179e-04,\n",
       "         4.69970703e-03, -1.58569336e-01, -7.80029297e-02, -1.67358398e-01,\n",
       "        -2.05688477e-01, -9.33837891e-02,  8.25805664e-02,  1.87744141e-01,\n",
       "        -2.30346680e-01, -1.88842773e-01, -5.00202179e-04,  1.96166992e-01,\n",
       "        -2.96936035e-02,  2.60498047e-01,  1.35131836e-01, -2.77832031e-01,\n",
       "         7.31201172e-02,  1.18225098e-01,  2.39257812e-01, -1.30249023e-01,\n",
       "         7.05078125e-01,  8.00323486e-03, -2.15942383e-01, -7.00073242e-02,\n",
       "         6.89697266e-02, -1.37939453e-01,  2.40844727e-01, -8.75854492e-02,\n",
       "        -1.37451172e-01, -5.10864258e-02,  4.73937988e-02, -1.73034668e-02,\n",
       "         2.96386719e-01,  7.67822266e-02, -5.19104004e-02,  3.19580078e-01,\n",
       "         1.09802246e-01, -5.05981445e-02, -1.48071289e-01,  7.64770508e-02,\n",
       "         1.09008789e-01, -3.17993164e-02, -1.14074707e-01, -1.51245117e-01,\n",
       "         1.75659180e-01, -3.53271484e-01,  2.76031494e-02,  3.82080078e-02,\n",
       "        -1.90429688e-01,  1.82006836e-01,  7.95898438e-02,  4.15283203e-01,\n",
       "        -1.49002075e-02, -2.60498047e-01, -1.31591797e-01,  3.52294922e-01,\n",
       "         1.63085938e-01, -1.21093750e-01, -4.36767578e-01, -9.41772461e-02,\n",
       "        -1.17614746e-01,  3.09944153e-03,  8.81958008e-02, -3.15185547e-01,\n",
       "         1.89971924e-03, -4.40979004e-02,  3.10302734e-01,  1.08032227e-02,\n",
       "        -5.75683594e-01,  1.12182617e-01,  1.37084961e-01, -6.30187988e-03,\n",
       "         9.59777832e-03,  9.82910156e-01, -1.05224609e-01,  9.66796875e-02]),\n",
       " array([ 8.83789062e-02,  1.50012970e-03,  3.24096680e-02,  1.34643555e-01,\n",
       "        -1.41723633e-01, -5.75866699e-02,  9.39941406e-03,  5.78002930e-02,\n",
       "         9.06982422e-02, -7.84912109e-02, -1.71966553e-02, -4.04052734e-02,\n",
       "        -4.58984375e-02,  8.11157227e-02, -2.72979736e-02,  3.53088379e-02,\n",
       "         7.86743164e-02,  3.58886719e-02,  6.80160522e-03, -9.36279297e-02,\n",
       "        -3.25012207e-02, -2.69012451e-02, -4.29916382e-03, -3.29971313e-03,\n",
       "         6.00891113e-02,  3.18908691e-02, -6.69860840e-03,  3.68957520e-02,\n",
       "        -4.40063477e-02,  1.17004395e-01,  2.07977295e-02,  1.14974976e-02,\n",
       "        -3.14941406e-02, -1.11572266e-01,  1.80053711e-02, -4.66003418e-02,\n",
       "         2.00080872e-03,  7.84301758e-02, -5.71899414e-02,  1.15966797e-02,\n",
       "         2.99987793e-02, -1.38015747e-02, -5.45043945e-02,  1.03576660e-01,\n",
       "        -3.70025635e-03, -1.47018433e-02, -1.00040436e-03,  2.69012451e-02,\n",
       "         1.59545898e-01, -2.50053406e-03, -8.49914551e-03,  9.80377197e-03,\n",
       "        -8.31298828e-02, -4.80041504e-02, -1.69982910e-02, -5.29861450e-03,\n",
       "        -4.32128906e-02,  7.30133057e-03, -8.81958008e-02, -5.40161133e-03,\n",
       "        -4.00066376e-04,  8.10241699e-03,  1.06964111e-02, -3.14941406e-02,\n",
       "         4.01916504e-02,  2.45056152e-02,  1.03210449e-01,  2.87017822e-02,\n",
       "         1.16821289e-01, -2.63061523e-02,  3.62854004e-02,  2.27050781e-02,\n",
       "        -4.44030762e-02, -6.80160522e-03,  3.61022949e-02,  1.29013062e-02,\n",
       "         3.67126465e-02,  2.16064453e-02,  4.90112305e-02,  2.28149414e-01,\n",
       "        -5.90133667e-03,  2.49938965e-02, -4.19921875e-02,  1.04980469e-02,\n",
       "         2.45056152e-02,  2.36053467e-02, -1.20010376e-02,  3.51867676e-02,\n",
       "        -1.50985718e-02, -3.05023193e-02, -5.15136719e-02, -8.00781250e-02,\n",
       "         3.05175781e-01, -4.50134277e-02, -2.65960693e-02,  2.25982666e-02,\n",
       "         1.19873047e-01,  1.06201172e-01,  4.15039062e-02,  5.20019531e-02,\n",
       "        -2.38952637e-02,  1.71966553e-02,  6.81152344e-02,  8.55102539e-02,\n",
       "        -8.34960938e-02,  2.74963379e-02, -1.05972290e-02, -3.13110352e-02,\n",
       "         1.69067383e-02,  1.14501953e-01,  2.65045166e-02, -8.08105469e-02,\n",
       "        -1.87988281e-02,  1.86279297e-01, -3.89099121e-02, -8.39996338e-03,\n",
       "        -3.40881348e-02, -1.24969482e-02,  3.78112793e-02, -4.47998047e-02,\n",
       "        -1.18026733e-02,  6.50024414e-03,  5.10025024e-03,  6.40106201e-03,\n",
       "         4.05883789e-02, -6.99996948e-04, -2.43988037e-02,  8.10241699e-03,\n",
       "        -4.04968262e-02,  5.46875000e-02,  4.08020020e-02, -1.15722656e-01,\n",
       "         1.58996582e-02,  1.06201172e-01, -5.38940430e-02, -1.93023682e-02,\n",
       "         5.63049316e-02, -1.11007690e-02,  2.07061768e-02, -4.91027832e-02,\n",
       "        -8.11157227e-02, -3.69873047e-02,  4.84008789e-02, -2.18963623e-02,\n",
       "         4.33959961e-02,  7.61718750e-02, -2.58789062e-01, -8.30078125e-03,\n",
       "        -3.75976562e-02, -6.62841797e-02, -1.64550781e-01,  5.73120117e-02,\n",
       "         6.67724609e-02,  8.75854492e-02, -5.20935059e-02, -9.30023193e-03,\n",
       "         1.54907227e-01,  5.42907715e-02,  3.39965820e-02, -1.69067383e-02,\n",
       "         8.17260742e-02,  6.17065430e-02,  6.70166016e-02, -3.22875977e-02,\n",
       "        -6.99996948e-04, -2.11029053e-02,  6.59942627e-03,  3.03039551e-02,\n",
       "         9.41162109e-02, -3.39965820e-02,  8.53271484e-02,  1.71051025e-02,\n",
       "        -3.68957520e-02, -4.14123535e-02,  2.79998779e-02, -5.10864258e-02,\n",
       "         9.20104980e-03, -2.43988037e-02,  2.09999084e-03, -3.58886719e-02,\n",
       "        -3.32031250e-02, -5.06896973e-02,  1.22985840e-02, -5.34973145e-02,\n",
       "         7.06787109e-02, -1.70410156e-01, -6.19888306e-03,  8.69750977e-03,\n",
       "         2.13928223e-02, -7.31811523e-02, -6.03942871e-02, -2.22015381e-02,\n",
       "        -3.61938477e-02,  5.35888672e-02, -5.74951172e-02,  8.22753906e-02,\n",
       "         9.64965820e-02,  7.06176758e-02,  3.24096680e-02,  9.44213867e-02,\n",
       "        -7.95898438e-02,  2.25341797e-01,  4.29916382e-03,  7.09915161e-03,\n",
       "        -1.18713379e-01, -7.40051270e-03, -1.01989746e-01, -5.78002930e-02,\n",
       "         5.90133667e-03,  1.51000977e-01, -7.80105591e-03, -5.96008301e-02,\n",
       "        -1.21002197e-02,  5.37109375e-02,  7.81250000e-02,  1.89056396e-02,\n",
       "        -1.05972290e-02, -2.67028809e-02,  2.11029053e-02, -2.42004395e-02,\n",
       "         4.55017090e-02,  2.23999023e-02, -1.41983032e-02, -1.38916016e-01,\n",
       "        -1.87988281e-02, -6.73217773e-02,  1.01318359e-01, -9.46044922e-02,\n",
       "         4.87060547e-02, -3.87878418e-02, -1.15966797e-02,  1.08032227e-02,\n",
       "        -1.43432617e-01, -7.40051270e-03, -2.92968750e-02, -1.49993896e-02,\n",
       "        -6.40869141e-02,  8.12988281e-02,  5.15136719e-02, -5.67016602e-02,\n",
       "         2.90069580e-02,  1.09024048e-02, -1.69982910e-02, -1.02905273e-01,\n",
       "         2.99804688e-01, -2.11944580e-02, -2.74047852e-02, -4.00066376e-04,\n",
       "         5.28869629e-02, -6.84204102e-02,  8.59832764e-03,  3.43017578e-02,\n",
       "        -1.51977539e-02, -1.42089844e-01, -2.02941895e-02,  5.90133667e-03,\n",
       "        -6.80160522e-03,  2.29034424e-02,  8.85009766e-02, -1.30996704e-02,\n",
       "        -2.74963379e-02, -8.80126953e-02,  4.48913574e-02,  7.73925781e-02,\n",
       "         8.42285156e-02, -2.76947021e-02,  6.10961914e-02, -1.57928467e-02,\n",
       "        -3.50952148e-02, -5.98144531e-02, -8.25195312e-02, -4.70886230e-02,\n",
       "        -3.87878418e-02, -1.39999390e-03, -1.50985718e-02, -3.97949219e-02,\n",
       "         6.13098145e-02, -7.25097656e-02,  1.57012939e-02, -6.76269531e-02,\n",
       "         3.40080261e-03, -1.08581543e-01, -1.95312500e-01, -6.40258789e-02,\n",
       "        -4.80041504e-02,  3.86047363e-02, -4.87976074e-02, -1.77368164e-01,\n",
       "         9.86938477e-02, -4.50134277e-02,  1.10015869e-02, -2.99930573e-04,\n",
       "        -7.78198242e-02,  4.94995117e-02, -5.77087402e-02, -2.31018066e-02,\n",
       "         5.12084961e-02,  1.98852539e-01, -4.45861816e-02, -1.08276367e-01]),\n",
       " array([ 0.00630188, -0.00419998,  0.0914917 ,  0.043396  , -0.04348755,\n",
       "        -0.00699997, -0.01429749, -0.02200317,  0.03988647, -0.00980377,\n",
       "        -0.01110077, -0.02990723, -0.05410767, -0.00370026,  0.03829956,\n",
       "        -0.00699997,  0.00040007, -0.02580261, -0.01759338, -0.00500107,\n",
       "        -0.03659058, -0.00129986, -0.01399994, -0.01930237, -0.01620483,\n",
       "         0.02160645,  0.01280212, -0.00550079,  0.01239777,  0.01239777,\n",
       "        -0.00359917, -0.0102005 , -0.02020264,  0.01780701,  0.0134964 ,\n",
       "        -0.01059723,  0.06109619,  0.01439667,  0.0171051 , -0.0017004 ,\n",
       "        -0.01110077,  0.0034008 ,  0.0082016 ,  0.01600647, -0.0308075 ,\n",
       "        -0.04058838,  0.01370239, -0.01750183, -0.01609802, -0.00219917,\n",
       "        -0.00650024,  0.02009583,  0.03430176,  0.01110077, -0.01239777,\n",
       "        -0.00930023,  0.01170349, -0.09967041, -0.0670166 ,  0.00020003,\n",
       "         0.02929688,  0.0134964 , -0.04071045, -0.04708862, -0.02229309,\n",
       "         0.00839996, -0.03320312, -0.02420044,  0.0322876 ,  0.01280212,\n",
       "        -0.04980469, -0.00690079,  0.02119446,  0.00359917, -0.01950073,\n",
       "         0.00529861,  0.06341553,  0.0082016 , -0.03359985, -0.06628418,\n",
       "         0.0041008 , -0.02099609,  0.02200317,  0.00500107, -0.02049255,\n",
       "         0.0089035 ,  0.01750183,  0.02099609,  0.01660156,  0.02729797,\n",
       "        -0.0243988 ,  0.00519943,  0.08441162, -0.02920532, -0.03518677,\n",
       "         0.02540588, -0.01110077,  0.04690552, -0.01069641, -0.00910187,\n",
       "        -0.02839661, -0.0034008 ,  0.01339722, -0.01229858, -0.00719833,\n",
       "        -0.00479889,  0.02729797, -0.02470398, -0.02670288, -0.00640106,\n",
       "        -0.03240967,  0.07092285, -0.00730133,  0.01360321,  0.00259972,\n",
       "        -0.03259277,  0.01809692,  0.02720642, -0.00450134, -0.00959778,\n",
       "        -0.05239868,  0.03059387, -0.04580688, -0.04431152,  0.02609253,\n",
       "         0.00439835, -0.00020003,  0.03900146,  0.00869751,  0.02870178,\n",
       "        -0.04000854, -0.04708862,  0.05041504,  0.04000854, -0.019104  ,\n",
       "        -0.01980591,  0.00600052,  0.05239868, -0.03610229, -0.02720642,\n",
       "        -0.03619385,  0.02029419,  0.01260376, -0.03019714,  0.04818726,\n",
       "         0.0141983 , -0.07867432,  0.02609253,  0.0109024 ,  0.05661011,\n",
       "        -0.06799316, -0.03741455,  0.00400162,  0.02209473, -0.01280212,\n",
       "        -0.0269928 ,  0.04541016, -0.0007    , -0.0041008 ,  0.03500366,\n",
       "        -0.00320053, -0.02000427, -0.04830933, -0.04458618, -0.00719833,\n",
       "         0.05151367,  0.03900146,  0.06130981, -0.00400162,  0.01180267,\n",
       "         0.00579834,  0.01029968, -0.01200104,  0.02589417, -0.0256958 ,\n",
       "         0.00740051, -0.02859497, -0.11297607,  0.04440308, -0.00910187,\n",
       "         0.0362854 ,  0.01750183, -0.01139832, -0.01159668, -0.0082016 ,\n",
       "        -0.00479889, -0.04650879, -0.022995  ,  0.01239777, -0.02920532,\n",
       "        -0.03768921,  0.0296936 ,  0.02189636,  0.02049255, -0.02189636,\n",
       "         0.0094986 , -0.02450562, -0.01200104,  0.04650879,  0.07769775,\n",
       "        -0.02890015,  0.05511475,  0.00680161, -0.06188965, -0.02819824,\n",
       "        -0.00040007,  0.03610229, -0.09350586,  0.0369873 ,  0.02799988,\n",
       "         0.08477783, -0.01919556,  0.02890015, -0.01939392,  0.02510071,\n",
       "         0.03781128, -0.0269928 , -0.02839661,  0.00279999, -0.03140259,\n",
       "         0.043396  ,  0.08032227,  0.00640106,  0.05059814, -0.01339722,\n",
       "        -0.02229309, -0.01620483, -0.03050232, -0.01470184,  0.01739502,\n",
       "         0.00139999, -0.00550079,  0.03381348,  0.02960205, -0.01699829,\n",
       "         0.03900146, -0.01960754, -0.06500244, -0.04400635, -0.00749969,\n",
       "         0.01159668,  0.00800323, -0.00730133, -0.06707764,  0.10638428,\n",
       "         0.01789856, -0.02960205,  0.02630615,  0.02589417, -0.00329971,\n",
       "        -0.01649475, -0.05960083,  0.03359985, -0.05801392,  0.01470184,\n",
       "         0.02220154, -0.01789856, -0.021698  ,  0.06567383,  0.00320053,\n",
       "        -0.02560425,  0.02270508,  0.02049255,  0.03430176, -0.01950073,\n",
       "         0.0486145 ,  0.02720642,  0.00150013,  0.00719833, -0.04708862,\n",
       "        -0.02009583, -0.00419998, -0.02720642,  0.02220154, -0.00109959,\n",
       "        -0.0546875 ,  0.06408691,  0.00789642, -0.04278564,  0.04089355,\n",
       "        -0.08087158,  0.00690079, -0.06896973,  0.00089979,  0.00619888,\n",
       "         0.00469971, -0.01470184, -0.07592773, -0.00830078, -0.01229858,\n",
       "         0.02780151,  0.02259827, -0.05569458,  0.02220154,  0.03820801,\n",
       "         0.03329468,  0.00189972,  0.03329468,  0.00439835, -0.00669861]),\n",
       " array([ 1.64031982e-02, -6.45751953e-02, -2.07977295e-02,  6.29882812e-02,\n",
       "        -4.01000977e-02,  2.81982422e-02,  1.08276367e-01, -2.49023438e-02,\n",
       "         1.19018555e-02, -2.00033188e-04, -2.96936035e-02,  2.13012695e-02,\n",
       "        -8.10241699e-03,  3.79943848e-03, -1.48010254e-02, -7.48291016e-02,\n",
       "         2.54974365e-02,  2.29949951e-02, -3.38134766e-02,  1.94335938e-01,\n",
       "        -5.62133789e-02,  1.39999390e-02, -4.37927246e-02, -1.47949219e-01,\n",
       "        -1.86523438e-01, -3.90930176e-02, -1.75933838e-02,  2.52990723e-02,\n",
       "        -5.59997559e-03, -6.48193359e-02, -9.63745117e-02, -2.99930573e-04,\n",
       "        -7.26928711e-02, -1.43920898e-01, -2.34985352e-02,  2.63977051e-02,\n",
       "         5.81054688e-02,  3.80859375e-02,  5.85937500e-02,  5.67016602e-02,\n",
       "        -8.66699219e-02, -1.42974854e-02,  7.12890625e-02,  2.72979736e-02,\n",
       "        -6.19888306e-03, -6.70166016e-02, -4.91943359e-02, -1.34155273e-01,\n",
       "         1.78985596e-02, -6.90078735e-03,  4.33959961e-02,  6.59790039e-02,\n",
       "        -4.08935547e-02, -2.89001465e-02,  4.16870117e-02,  3.46984863e-02,\n",
       "         6.10046387e-02, -1.49169922e-01, -1.73950195e-01,  1.12991333e-02,\n",
       "         2.45056152e-02, -2.22930908e-02, -7.83081055e-02, -3.44848633e-02,\n",
       "        -8.36791992e-02, -2.58026123e-02, -7.30133057e-03, -7.89642334e-03,\n",
       "         5.92041016e-02, -9.90295410e-03,  8.28857422e-02,  3.59916687e-03,\n",
       "         2.11944580e-02, -5.71899414e-02,  1.43966675e-02,  3.90052795e-03,\n",
       "         4.91943359e-02,  8.02001953e-02,  1.47460938e-01, -9.35058594e-02,\n",
       "         2.61993408e-02,  4.73022461e-02, -1.41983032e-02,  8.10241699e-03,\n",
       "         9.38110352e-02,  3.72924805e-02,  1.55639648e-01,  7.01293945e-02,\n",
       "        -1.62963867e-02,  1.42578125e-01, -8.49914551e-03,  3.03039551e-02,\n",
       "        -2.40936279e-02,  3.67126465e-02, -6.62231445e-02,  1.80053711e-02,\n",
       "         8.72192383e-02, -1.14974976e-02,  2.54058838e-02,  3.21960449e-02,\n",
       "         4.79888916e-03, -4.19921875e-02, -1.60026550e-03,  4.11987305e-02,\n",
       "        -4.60052490e-03, -2.89916992e-03, -4.55932617e-02, -5.10025024e-03,\n",
       "        -5.00202179e-04, -9.69696045e-03,  6.40869141e-02,  1.21215820e-01,\n",
       "        -1.06964111e-02,  2.00805664e-01,  4.14123535e-02, -8.05053711e-02,\n",
       "        -3.90052795e-03, -7.49969482e-03,  1.21994019e-02, -2.34069824e-02,\n",
       "         5.49926758e-02,  2.81066895e-02, -5.52062988e-02,  4.39834595e-03,\n",
       "        -3.86047363e-02,  8.39233398e-02, -2.05993652e-02,  1.97998047e-01,\n",
       "         5.59997559e-03,  1.13983154e-02, -2.85949707e-02, -2.20947266e-02,\n",
       "        -2.99072266e-02,  4.55017090e-02,  4.33959961e-02, -2.49023438e-02,\n",
       "        -4.26025391e-02,  9.20104980e-03, -1.95007324e-02,  4.00066376e-04,\n",
       "         7.80105591e-03,  1.82037354e-02,  4.66003418e-02, -1.67999268e-02,\n",
       "        -7.53173828e-02,  7.06787109e-02, -3.51806641e-01, -1.43966675e-02,\n",
       "         3.28063965e-02,  7.40051270e-03, -1.08825684e-01,  3.20129395e-02,\n",
       "         2.16064453e-02,  1.70040131e-03, -6.76879883e-02, -2.52075195e-02,\n",
       "         2.09838867e-01,  2.87933350e-02, -5.05065918e-02,  1.71966553e-02,\n",
       "        -3.57971191e-02,  8.88061523e-02,  2.56042480e-02,  1.10015869e-02,\n",
       "        -6.03027344e-02,  1.27029419e-02,  2.67028809e-02, -1.25244141e-01,\n",
       "        -2.11944580e-02,  3.67126465e-02, -1.09024048e-02, -1.13983154e-02,\n",
       "         2.36053467e-02, -2.43988037e-02, -5.10025024e-03,  2.92053223e-02,\n",
       "        -9.36889648e-02, -1.51489258e-01, -4.29916382e-03, -1.80053711e-02,\n",
       "         8.67919922e-02, -6.76269531e-02,  2.85949707e-02, -3.44848633e-02,\n",
       "         2.83050537e-02, -5.75866699e-02, -2.23632812e-01, -9.85107422e-02,\n",
       "         7.25097656e-02, -5.89904785e-02, -1.42974854e-02,  1.28021240e-02,\n",
       "         7.53173828e-02, -7.30133057e-03,  3.90052795e-03,  2.36053467e-02,\n",
       "         3.08990479e-02,  4.04968262e-02,  3.79943848e-02,  2.81738281e-01,\n",
       "        -1.00402832e-01,  1.95068359e-01, -6.90917969e-02, -5.15136719e-02,\n",
       "        -2.98004150e-02,  2.11944580e-02, -6.14929199e-02, -1.50024414e-01,\n",
       "         7.86132812e-02,  2.21679688e-01, -9.06982422e-02,  1.32980347e-02,\n",
       "         1.68212891e-01,  2.38952637e-02,  9.80834961e-02, -6.00051880e-03,\n",
       "        -2.79998779e-02, -2.13012695e-02,  4.00066376e-04,  1.46362305e-01,\n",
       "         9.41162109e-02, -5.66101074e-02,  3.71093750e-02,  4.08020020e-02,\n",
       "         1.51977539e-02,  1.75018311e-02, -5.90133667e-03,  3.09944153e-03,\n",
       "        -7.55004883e-02,  2.14996338e-02, -1.37023926e-02, -5.26123047e-02,\n",
       "         5.27954102e-02,  7.29980469e-02,  5.40161133e-03,  1.28051758e-01,\n",
       "         2.67028809e-02, -2.29949951e-02, -1.32980347e-02,  3.28979492e-02,\n",
       "        -3.44848633e-02,  1.61254883e-01,  2.74047852e-02, -1.76269531e-01,\n",
       "         5.31250000e-01,  4.00066376e-04,  2.56042480e-02, -1.06964111e-02,\n",
       "         8.49914551e-03, -1.75659180e-01, -6.79931641e-02,  1.82952881e-02,\n",
       "        -6.59942627e-03, -4.50134277e-02,  4.00161743e-03,  2.34069824e-02,\n",
       "        -6.22863770e-02,  1.30996704e-02,  1.96289062e-01,  6.80160522e-03,\n",
       "         4.69970703e-02, -4.29992676e-02, -2.99930573e-04,  3.62854004e-02,\n",
       "        -2.44018555e-01, -1.56021118e-02,  5.00202179e-04,  4.00066376e-04,\n",
       "         2.25097656e-01, -1.24694824e-01, -1.22985840e-02, -1.71966553e-02,\n",
       "        -6.10961914e-02, -9.17968750e-02, -1.27075195e-01, -1.21398926e-01,\n",
       "        -6.37817383e-02, -2.06542969e-01, -5.81054688e-02,  3.50585938e-01,\n",
       "        -1.63452148e-01,  5.42907715e-02, -9.10034180e-02,  6.69860840e-03,\n",
       "        -1.71966553e-02,  4.29916382e-03,  7.89794922e-02, -1.13220215e-01,\n",
       "        -4.19921875e-02,  6.90078735e-03,  5.62133789e-02,  6.79931641e-02,\n",
       "        -2.90069580e-02, -8.91113281e-02,  6.17980957e-02,  1.22985840e-02,\n",
       "        -2.83966064e-02, -3.54919434e-02,  1.78985596e-02,  3.85131836e-02]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_vec = np.mean([w for w in ft_en.values()], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "eng_span[\"English_Emb\"] = eng_span[\"en_sent\"].apply(\n",
    "    lambda x: sentence_to_vector(x, ft_en,default_vec)\n",
    ")\n",
    "eng_span[\"Spanish_Emb\"] = eng_span[\"span_sent\"].apply(\n",
    "    lambda x: sentence_to_vector(x, ft_es,default_vec)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"✅ Embeddings created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_span[\"English_Emb\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_port[\"English_Emb\"] = eng_port[\"en_sent\"].apply(lambda x: sentence_to_vector(x, ft_en,default_vec))\n",
    "eng_port[\"Portuguese_Emb\"] = eng_port[\"port_sent\"].apply(lambda x: sentence_to_vector(x, ft_pt,default_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ft_es\n",
    "del ft_en\n",
    "del ft_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_span.to_pickle(\"eng_span.pkl\")\n",
    "eng_port.to_pickle(\"eng_port.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del eng_span\n",
    "del eng_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_file(data,file):\n",
    "    for i in range(0,len(data),30000):\n",
    "        n = i // 30000\n",
    "        data[i:30000+i].to_pickle(f\"{file}_{n}.pkl\")\n",
    "        print(f\"saved train {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_port = pd.read_pickle(\"eng_port.pkl\")\n",
    "eng_span = pd.read_pickle(\"eng_span.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved train 0\n",
      "saved train 1\n",
      "saved train 2\n",
      "saved train 3\n",
      "saved train 4\n",
      "saved train 5\n",
      "saved train 6\n",
      "saved train 7\n",
      "saved train 8\n",
      "saved train 9\n"
     ]
    }
   ],
   "source": [
    "chunk_file(eng_port,\"./train_en_pt/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved train 0\n",
      "saved train 1\n",
      "saved train 2\n",
      "saved train 3\n",
      "saved train 4\n",
      "saved train 5\n",
      "saved train 6\n",
      "saved train 7\n",
      "saved train 8\n",
      "saved train 9\n"
     ]
    }
   ],
   "source": [
    "chunk_file(eng_span,\"./train_en_es/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_port = pd.read_pickle(\"./train_en_pt/train_0.pkl\")\n",
    "eng_span = pd.read_pickle(\"./train_en_es/train_0.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['en_no', 'en_sent', 'port_no', 'port_sent', 'English_Emb',\n",
      "       'Portuguese_Emb', 'Portugese_Emb'],\n",
      "      dtype='object')\n",
      "Index(['span_no', 'span_sent', 'en_no', 'en_sent', 'English_Emb',\n",
      "       'Spanish_Emb'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(eng_port.columns)\n",
    "print(eng_span.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def apply_pca(embeddings):\n",
    "    # Step 1: Flatten all embeddings into one matrix\n",
    "    all_embeddings = []\n",
    "    sentence_lengths = []\n",
    "    for sentence_emb in embeddings:\n",
    "        all_embeddings.append(sentence_emb)  # List of (num_words, 300) arrays\n",
    "        sentence_lengths.append(sentence_emb.shape[0])  # Track num_words per sentence\n",
    "\n",
    "    # Concatenate into a single (N_total_words, 300) matrix\n",
    "    X = np.vstack(all_embeddings)\n",
    "    print(\"Original shape (all words):\", X.shape)  # e.g., (32, 300) for 14 + 10 + 8 words\n",
    "\n",
    "    # Step 2: Center the data (optional but recommended)\n",
    "    X_centered = X - np.mean(X, axis=0)\n",
    "\n",
    "    # Step 3: Apply PCA\n",
    "    # Option A: Reduce to 150D (like the paper)\n",
    "    pca_150 = PCA(n_components=150)\n",
    "\n",
    "    # Check explained variance (how much info is retained)\n",
    "    X_150d = pca_150.fit_transform(X_centered)\n",
    "    print(\"Explained variance ratio (150D):\", np.sum(pca_150.explained_variance_ratio_))\n",
    "    print(\"Reduced to 150D shape:\", X_150d.shape)  # e.g., (32, 150)\n",
    "    return X_150d,sentence_lengths\n",
    "\n",
    "\n",
    "# Step 4: Reconstruct sentence-level embeddings\n",
    "def reconstruct_sentences(reduced_emb, sentence_lengths):\n",
    "    reduced_sentences = []\n",
    "    start_idx = 0\n",
    "    for length in sentence_lengths:\n",
    "        end_idx = start_idx + length\n",
    "        reduced_sentences.append(reduced_emb[start_idx:end_idx, :])\n",
    "        start_idx = end_idx\n",
    "    return reduced_sentences\n",
    "\n",
    "\n",
    "def apply_pca_recontruct(embeddings):\n",
    "    return reconstruct_sentences(*apply_pca(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape (all words): (232472, 300)\n",
      "Explained variance ratio (150D): 0.9736280668180937\n",
      "Reduced to 150D shape: (232472, 150)\n",
      "Original shape (all words): (226014, 300)\n",
      "Explained variance ratio (150D): 0.9661872891762826\n",
      "Reduced to 150D shape: (226014, 150)\n"
     ]
    }
   ],
   "source": [
    "eng_port[\"English_Emb\"] = apply_pca_recontruct(eng_port[\"English_Emb\"])\n",
    "eng_port[\"Portugese_Emb\"] = apply_pca_recontruct(eng_port[\"Portuguese_Emb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape (all words): (213534, 300)\n",
      "Explained variance ratio (150D): 0.9744586049345564\n",
      "Reduced to 150D shape: (213534, 150)\n",
      "Original shape (all words): (198313, 300)\n",
      "Explained variance ratio (150D): 0.9714751449631979\n",
      "Reduced to 150D shape: (198313, 150)\n"
     ]
    }
   ],
   "source": [
    "eng_span[\"English_Emb\"] = apply_pca_recontruct(eng_span[\"English_Emb\"])\n",
    "eng_span[\"Spanish_Emb\"] = apply_pca_recontruct(eng_span[\"Spanish_Emb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training pairs: 60000\n",
      "Sample input: <2EN> Podría también alquilar un coche.\n",
      "Sample target: You could also hire a car.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def prepare_language_pair(\n",
    "    df, source_col, target_col, source_emb_col, target_emb_col, target_token\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepare input-target pairs for a language pair with target language token.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the language pair data.\n",
    "        source_col (str): Column name for source sentences.\n",
    "        target_col (str): Column name for target sentences.\n",
    "        source_emb_col (str): Column name for source embeddings.\n",
    "        target_emb_col (str): Column name for target embeddings.\n",
    "        target_token (str): Token to prepend to source sentences (e.g., '<2EN>').\n",
    "\n",
    "    Returns:\n",
    "        tuple: (inputs, targets, input_emb, target_emb) as lists.\n",
    "    \"\"\"\n",
    "    inputs = [f\"{target_token} \" + row[source_col] for _, row in df.iterrows()]\n",
    "    targets = [row[target_col] for _, row in df.iterrows()]\n",
    "    input_emb = df[source_emb_col].tolist()\n",
    "    target_emb = df[target_emb_col].tolist()\n",
    "\n",
    "    return inputs, targets, input_emb, target_emb\n",
    "\n",
    "\n",
    "def combine_language_pairs(*pairs):\n",
    "    \"\"\"\n",
    "    Combine multiple language pairs into a single dataset.\n",
    "\n",
    "    Args:\n",
    "        *pairs: Variable number of tuples, each containing (inputs, targets, input_emb, target_emb).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Combined (all_inputs, all_targets, all_input_emb, all_target_emb).\n",
    "    \"\"\"\n",
    "    all_inputs = []\n",
    "    all_targets = []\n",
    "    all_input_emb = []\n",
    "    all_target_emb = []\n",
    "\n",
    "    for inputs, targets, input_emb, target_emb in pairs:\n",
    "        all_inputs.extend(inputs)\n",
    "        all_targets.extend(targets)\n",
    "        all_input_emb.extend(input_emb)\n",
    "        all_target_emb.extend(target_emb)\n",
    "\n",
    "    return all_inputs, all_targets, all_input_emb, all_target_emb\n",
    "\n",
    "\n",
    "# Example usage with your DataFrames\n",
    "def prepare_zst_data(eng_span, eng_port):\n",
    "    \"\"\"\n",
    "    Prepare data for ZST model using Spanish-English and English-Portuguese DataFrames.\n",
    "\n",
    "    Args:\n",
    "        eng_span (pd.DataFrame): Spanish-English data.\n",
    "        eng_port (pd.DataFrame): English-Portuguese data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Combined data (all_inputs, all_targets, all_input_emb, all_target_emb).\n",
    "    \"\"\"\n",
    "    # Prepare Spanish-English pair\n",
    "    sp_en_data = prepare_language_pair(\n",
    "        df=eng_span,\n",
    "        source_col=\"span_sent\",\n",
    "        target_col=\"en_sent\",\n",
    "        source_emb_col=\"Spanish_Emb\",\n",
    "        target_emb_col=\"English_Emb\",\n",
    "        target_token=\"<2EN>\",\n",
    "    )\n",
    "\n",
    "    # Prepare English-Portuguese pair\n",
    "    en_pr_data = prepare_language_pair(\n",
    "        df=eng_port,\n",
    "        source_col=\"en_sent\",\n",
    "        target_col=\"port_sent\",\n",
    "        source_emb_col=\"English_Emb\",\n",
    "        target_emb_col=\"Portuguese_Emb\",\n",
    "        target_token=\"<2PR>\",\n",
    "    )\n",
    "\n",
    "    # Combine all pairs\n",
    "    combined_data = combine_language_pairs(sp_en_data, en_pr_data)\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "\n",
    "# Assuming eng_span and eng_port are your DataFrames\n",
    "# Run the preparation\n",
    "all_inputs, all_targets, all_input_emb, all_target_emb = prepare_zst_data(\n",
    "    eng_span, eng_port\n",
    ")\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"Total training pairs:\", len(all_inputs))\n",
    "print(\"Sample input:\", all_inputs[0])\n",
    "print(\"Sample target:\", all_targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (60000,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m all_inputs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_inputs)\n\u001b[1;32m      2\u001b[0m all_targets \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_targets)\n\u001b[0;32m----> 3\u001b[0m all_input_emb \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_input_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m all_target_emb \u001b[38;5;241m=\u001b[39m  np\u001b[38;5;241m.\u001b[39marray(all_target_emb)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (60000,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "all_inputs = np.array(all_inputs)\n",
    "all_targets = np.array(all_targets)\n",
    "all_input_emb = np.array(all_input_emb)\n",
    "all_target_emb =  np.array(all_target_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', lower=False, oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(all_inputs+all_targets)\n",
    "\n",
    "# Convert target sentences to sequences\n",
    "input_seqs = tokenizer.texts_to_sequences(all_inputs)\n",
    "target_seqs = tokenizer.texts_to_sequences(all_targets)\n",
    "\n",
    "# Add <START> and <END> tokens\n",
    "# start_token = tokenizer.word_index['<start>'] = len(tokenizer.word_index) + 1\n",
    "# end_token = tokenizer.word_index['<end>'] = len(tokenizer.word_index) + 2\n",
    "\n",
    "# target_sequences = [[start_token] + seq + [end_token] for seq in target_sequences]\n",
    "\n",
    "# Pad sequences to make them the same length\n",
    "input_seqs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    input_seqs, maxlen=50, padding=\"post\"\n",
    ")\n",
    "target_seqs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    target_seqs, maxlen=50, padding=\"post\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_data = target_seqs[:, :-1]\n",
    "decoder_target_data = np.expand_dims(target_seqs[:, 1:], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  56  242  639 5501    4  419    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0]\n",
      "[  56  242  639 5501    4  419    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(target_seqs[0,:-1])\n",
    "print(target_seqs[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270677, 150)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 256  # Word embedding dimension\n",
    "LSTM_UNITS = 512  # LSTM hidden units\n",
    "VOCAB_SIZE = 10000  # Vocabulary size\n",
    "MAX_LEN = 20  # Max sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = Bidirectional(\n",
    "            LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        output, forward_h, forward_c, backward_h, backward_c = self.lstm(x)\n",
    "        # Combine forward and backward states\n",
    "        state_h = tf.concat([forward_h, backward_h], axis=-1)\n",
    "        state_c = tf.concat([forward_c, backward_c], axis=-1)\n",
    "        return output, state_h, state_c  # Return sequence outputs and final state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)  # Encoder output transformation\n",
    "        self.W2 = Dense(units)  # Decoder hidden state transformation\n",
    "        self.V = Dense(1)  # Attention score\n",
    "\n",
    "    def call(self, encoder_output, decoder_hidden):\n",
    "        # Expand decoder hidden state for broadcasting\n",
    "        decoder_hidden_with_time_axis = tf.expand_dims(decoder_hidden, 1)\n",
    "        # Calculate attention scores\n",
    "        score = self.V(\n",
    "            tf.nn.tanh(self.W1(encoder_output) + self.W2(decoder_hidden_with_time_axis))\n",
    "        )\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)  # Softmax across time axis\n",
    "        context_vector = attention_weights * encoder_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)  # Sum across time axis\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = Bidirectional(\n",
    "            LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "        )\n",
    "        self.fc = Dense(vocab_size, activation=\"softmax\")\n",
    "        self.attention = BahdanauAttention(lstm_units)\n",
    "\n",
    "    def call(self, inputs, encoder_output, hidden_state):\n",
    "        x = self.embedding(inputs)\n",
    "        context_vector, attention_weights = self.attention(encoder_output, hidden_state)\n",
    "        # Combine context vector and input\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, forward_h, forward_c, backward_h, backward_c = self.lstm(x)\n",
    "        state_h = tf.concat([forward_h, backward_h], axis=-1)\n",
    "        state_c = tf.concat([forward_c, backward_c], axis=-1)\n",
    "        output = self.fc(output)  # Convert to vocabulary predictions\n",
    "        return output, state_h, state_c, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, lstm_units):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(vocab_size, embedding_dim, lstm_units)\n",
    "        self.decoder = Decoder(vocab_size, embedding_dim, lstm_units)\n",
    "\n",
    "    def call(self, encoder_input, decoder_input):\n",
    "        encoder_output, encoder_h, encoder_c = self.encoder(encoder_input)\n",
    "        decoder_output, _, _, _ = self.decoder(decoder_input, encoder_output, encoder_h)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = np.random.randint(0, VOCAB_SIZE, (64, MAX_LEN))  # (batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 20)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "missing a required argument: 'decoder_input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m decoder_target \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, VOCAB_SIZE, (\u001b[38;5;241m64\u001b[39m, MAX_LEN))  \u001b[38;5;66;03m# Shifted targets\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mencoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# model.fit([encoder_input, decoder_input], decoder_input, epochs=10, batch_size=64)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/deep_learning/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/lib/python3.11/inspect.py:3211\u001b[0m, in \u001b[0;36mSignature.bind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[1;32m   3208\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[1;32m   3209\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[1;32m   3210\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/inspect.py:3126\u001b[0m, in \u001b[0;36mSignature._bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3124\u001b[0m                 msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing a required argument: \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   3125\u001b[0m                 msg \u001b[38;5;241m=\u001b[39m msg\u001b[38;5;241m.\u001b[39mformat(arg\u001b[38;5;241m=\u001b[39mparam\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m-> 3126\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3128\u001b[0m     \u001b[38;5;66;03m# We have a positional argument to process\u001b[39;00m\n\u001b[1;32m   3129\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: missing a required argument: 'decoder_input'"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = Seq2Seq(VOCAB_SIZE, EMBEDDING_DIM, LSTM_UNITS)\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Generate dummy data (replace with actual dataset)\n",
    "encoder_input = np.random.randint(0, VOCAB_SIZE, (64, MAX_LEN))  # (batch_size, seq_len)\n",
    "decoder_input = np.random.randint(0, VOCAB_SIZE, (64, MAX_LEN))  # (batch_size, seq_len)\n",
    "decoder_target = np.random.randint(0, VOCAB_SIZE, (64, MAX_LEN))  # Shifted targets\n",
    "\n",
    "# Train the model\n",
    "model.fit([encoder_input, decoder_input], decoder_target, epochs=10, batch_size=64)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "# model.fit([encoder_input, decoder_input], decoder_input, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1741364379.928820    5575 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4309 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 6GB Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim=150, hidden_units=256):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.lstm = Bidirectional(LSTM(hidden_units, return_sequences=True, return_state=True))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder.\n",
    "        inputs: (batch_size, max_seq_length, embedding_dim)\n",
    "        \"\"\"\n",
    "        encoder_outputs, forward_h, forward_c, backward_h, backward_c = self.lstm(inputs)\n",
    "        print(\"backward\",backward_c.shape,backward_h.shape)\n",
    "\n",
    "        # Concatenate forward and backward hidden states\n",
    "        encoder_hidden = tf.concat([forward_h, backward_h], axis=1)\n",
    "        encoder_cell = tf.concat([forward_c, backward_c], axis=1)\n",
    "\n",
    "        return encoder_outputs, encoder_hidden, encoder_cell\n",
    "\n",
    "# Example usage\n",
    "batch_size = 32\n",
    "max_seq_length = 20  # Assuming max length per sentence\n",
    "embedding_dim = 150\n",
    "hidden_units = 256\n",
    "\n",
    "encoder = Encoder(embedding_dim=embedding_dim, hidden_units=hidden_units)\n",
    "\n",
    "# Dummy input (batch_size, max_seq_length, embedding_dim)\n",
    "# dummy_input = tf.random.normal((batch_size, max_seq_length, embedding_dim))\n",
    "\n",
    "# Run through encoder\n",
    "# encoder_outputs, encoder_hidden, encoder_cell = encoder(dummy_input)\n",
    "\n",
    "# print(f\"Encoder Outputs Shape: {encoder_outputs.shape}\")  # (batch_size, max_seq_length, hidden_units * 2)\n",
    "# print(f\"Encoder Hidden State Shape: {encoder_hidden.shape}\")  # (batch_size, hidden_units * 2)\n",
    "# print(f\"Encoder Cell State Shape: {encoder_cell.shape}\")  # (batch_size, hidden_units * 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mencoder\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m dummy_input\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m encoder_outputs\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "del encoder\n",
    "del dummy_input\n",
    "del encoder_outputs\n",
    "del encoder_hidden\n",
    "del encoder_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Attention\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim=150, hidden_units=256, vocab_size=10000):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.lstm = LSTM(hidden_units * 2, return_sequences=True, return_state=True)\n",
    "        self.attention = Attention()\n",
    "        self.dense = Dense(vocab_size, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, hidden_state, cell_state):\n",
    "        \"\"\"\n",
    "        Decoder forward pass with attention.\n",
    "        inputs: (batch_size, 1, embedding_dim)  # Previous word embedding\n",
    "        encoder_outputs: (batch_size, max_seq_length, hidden_units * 2)\n",
    "        hidden_state, cell_state: Initial states from encoder\n",
    "        \"\"\"\n",
    "        # Apply Attention: Aligns encoder outputs with current decoder state\n",
    "        context_vector = self.attention([hidden_state[:, tf.newaxis, :], encoder_outputs])\n",
    "\n",
    "        # 🛠 Fix: Expand context vector across all time steps\n",
    "        time_steps = inputs.shape[1]  # Should be 49\n",
    "        context_vector = tf.repeat(context_vector, repeats=time_steps, axis=1)\n",
    "\n",
    "        # LSTM Forward Pass\n",
    "        inputs = tf.cast(inputs, tf.float16)\n",
    "        lstm_output, new_hidden, new_cell = self.lstm(inputs, initial_state=[hidden_state, cell_state])\n",
    "\n",
    "        # Concatenate LSTM output with repeated context vector\n",
    "        concat_output = tf.concat([lstm_output, context_vector], axis=-1)\n",
    "\n",
    "        # Generate word probabilities\n",
    "        output = self.dense(concat_output)\n",
    "\n",
    "        return output, new_hidden, new_cell\n",
    "\n",
    "# Example usage\n",
    "decoder = Decoder(hidden_units=256, vocab_size=10000)\n",
    "\n",
    "# Dummy inputs\n",
    "# prev_word_embedding = tf.random.normal((32, 1, 150))  # (batch_size, 1, embedding_dim)\n",
    "# decoder_outputs, new_hidden, new_cell = decoder(prev_word_embedding, encoder_outputs, encoder_hidden, encoder_cell)\n",
    "\n",
    "# print(f\"Decoder Output Shape: {decoder_outputs.shape}\")  # (batch_size, 1, vocab_size)\n",
    "# print(f\"New Hidden State Shape: {new_hidden.shape}\")  # (batch_size, hidden_units * 2)\n",
    "# print(f\"New Cell State Shape: {new_cell.shape}\")  # (batch_size, hidden_units * 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_embeddings(batch_emb, max_seq_length, embedding_dim):\n",
    "    \"\"\"\n",
    "    Pads or truncates embeddings to ensure fixed shape (batch_size, max_seq_length, embedding_dim).\n",
    "    \"\"\"\n",
    "    padded_batch = []\n",
    "\n",
    "    for emb in batch_emb:\n",
    "        if len(emb) < max_seq_length:\n",
    "            # Pad with zeros\n",
    "            pad_width = max_seq_length - len(emb)\n",
    "            padded_emb = np.pad(emb, ((0, pad_width), (0, 0)), mode=\"constant\")\n",
    "        else:\n",
    "            # Truncate\n",
    "            padded_emb = emb[:max_seq_length]\n",
    "\n",
    "        padded_batch.append(padded_emb)\n",
    "\n",
    "    return np.array(padded_batch)  # Shape: (batch_size, max_seq_length, embedding_dim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward (32, 256) (32, 256)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_5575/3903943014.py\", line 32, in train_step  *\n        loss = loss_object(decoder_targets[:, t], predictions)\n    File \"/home/admin/Desktop/deep_learning/.venv/lib/python3.11/site-packages/keras/src/losses/loss.py\", line 67, in __call__  **\n        losses = self.call(y_true, y_pred)\n    File \"/home/admin/Desktop/deep_learning/.venv/lib/python3.11/site-packages/keras/src/losses/losses.py\", line 33, in call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/admin/Desktop/deep_learning/.venv/lib/python3.11/site-packages/keras/src/losses/losses.py\", line 2246, in sparse_categorical_crossentropy\n        res = ops.sparse_categorical_crossentropy(\n    File \"/home/admin/Desktop/deep_learning/.venv/lib/python3.11/site-packages/keras/src/ops/nn.py\", line 1963, in sparse_categorical_crossentropy\n        return backend.nn.sparse_categorical_crossentropy(\n    File \"/home/admin/Desktop/deep_learning/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/nn.py\", line 725, in sparse_categorical_crossentropy\n        raise ValueError(\n\n    ValueError: Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(32,), output.shape=(32, 49, 10000)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m     batch_targets \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(batch_targets, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# Train step\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_targets\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_target\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(all_inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/deep_learning/.venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filel02iul86.py:46\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(encoder_inputs, decoder_inputs, decoder_targets)\u001b[0m\n\u001b[1;32m     44\u001b[0m     decoder_input \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder_input\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     45\u001b[0m     t \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_targets\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecoder_cell\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecoder_hidden\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miterate_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m gradients \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(total_loss), ag__\u001b[38;5;241m.\u001b[39mld(encoder)\u001b[38;5;241m.\u001b[39mtrainable_variables \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(decoder)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     48\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(optimizer)\u001b[38;5;241m.\u001b[39mapply_gradients, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mzip\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(gradients), ag__\u001b[38;5;241m.\u001b[39mld(encoder)\u001b[38;5;241m.\u001b[39mtrainable_variables \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(decoder)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filel02iul86.py:26\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     24\u001b[0m t \u001b[38;5;241m=\u001b[39m itr\n\u001b[1;32m     25\u001b[0m predictions, decoder_hidden, decoder_cell \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(decoder), (ag__\u001b[38;5;241m.\u001b[39mld(decoder_inputs), ag__\u001b[38;5;241m.\u001b[39mld(encoder_outputs), ag__\u001b[38;5;241m.\u001b[39mld(decoder_hidden), ag__\u001b[38;5;241m.\u001b[39mld(decoder_cell)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 26\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_object\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_targets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(total_loss)\n\u001b[1;32m     28\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(tf\u001b[38;5;241m.\u001b[39mreduce_mean, (loss,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m~/Desktop/deep_learning/.venv/lib/python3.11/site-packages/keras/src/losses/loss.py:67\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m     60\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype), y_pred\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     63\u001b[0m y_true \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype), y_true\n\u001b[1;32m     65\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m out_mask \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mget_keras_mask(losses)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m out_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/deep_learning/.venv/lib/python3.11/site-packages/keras/src/losses/losses.py:33\u001b[0m, in \u001b[0;36mLossFunctionWrapper.call\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m     31\u001b[0m y_true \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure_up_to(y_true, \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m], y_true_y_pred)\n\u001b[1;32m     32\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure_up_to(y_pred, \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], y_true_y_pred)\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/deep_learning/.venv/lib/python3.11/site-packages/keras/src/losses/losses.py:2246\u001b[0m, in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(y_true, y_pred, from_logits, ignore_class, axis)\u001b[0m\n\u001b[1;32m   2241\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m y_true \u001b[38;5;241m*\u001b[39m ops\u001b[38;5;241m.\u001b[39mcast(valid_mask, y_true\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m   2242\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m y_pred \u001b[38;5;241m*\u001b[39m ops\u001b[38;5;241m.\u001b[39mcast(\n\u001b[1;32m   2243\u001b[0m         ops\u001b[38;5;241m.\u001b[39mexpand_dims(valid_mask, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), y_pred\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m   2244\u001b[0m     )\n\u001b[0;32m-> 2246\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_categorical_crossentropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2251\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2254\u001b[0m     valid_mask \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mreshape(valid_mask, res_shape)\n",
      "File \u001b[0;32m~/Desktop/deep_learning/.venv/lib/python3.11/site-packages/keras/src/ops/nn.py:1963\u001b[0m, in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m   1959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((target, output)):\n\u001b[1;32m   1960\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparseCategoricalCrossentropy(\n\u001b[1;32m   1961\u001b[0m         from_logits\u001b[38;5;241m=\u001b[39mfrom_logits, axis\u001b[38;5;241m=\u001b[39maxis\n\u001b[1;32m   1962\u001b[0m     )\u001b[38;5;241m.\u001b[39msymbolic_call(target, output)\n\u001b[0;32m-> 1963\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_categorical_crossentropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\n\u001b[1;32m   1965\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/deep_learning/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/nn.py:725\u001b[0m, in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `output` must be at least rank 1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    721\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    723\u001b[0m     )\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m--> 725\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    726\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `output` must have rank (ndim) `target.ndim - 1`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    727\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    728\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    729\u001b[0m     )\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_5575/3903943014.py\", line 32, in train_step  *\n        loss = loss_object(decoder_targets[:, t], predictions)\n    File \"/home/admin/Desktop/deep_learning/.venv/lib/python3.11/site-packages/keras/src/losses/loss.py\", line 67, in __call__  **\n        losses = self.call(y_true, y_pred)\n    File \"/home/admin/Desktop/deep_learning/.venv/lib/python3.11/site-packages/keras/src/losses/losses.py\", line 33, in call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/admin/Desktop/deep_learning/.venv/lib/python3.11/site-packages/keras/src/losses/losses.py\", line 2246, in sparse_categorical_crossentropy\n        res = ops.sparse_categorical_crossentropy(\n    File \"/home/admin/Desktop/deep_learning/.venv/lib/python3.11/site-packages/keras/src/ops/nn.py\", line 1963, in sparse_categorical_crossentropy\n        return backend.nn.sparse_categorical_crossentropy(\n    File \"/home/admin/Desktop/deep_learning/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/nn.py\", line 725, in sparse_categorical_crossentropy\n        raise ValueError(\n\n    ValueError: Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(32,), output.shape=(32, 49, 10000)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "MAX_SEQ_LENGTH = 50\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Define Optimizer & Loss Function\n",
    "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction=\"none\")\n",
    "\n",
    "@tf.function\n",
    "def train_step(encoder_inputs, decoder_inputs, decoder_targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Encode input\n",
    "        # encoder_inputs = tf.expand_dims(encoder_inputs,1)\n",
    "        # print(\"ff\",encoder_inputs.shape)\n",
    "        encoder_outputs, encoder_hidden, encoder_cell = encoder(encoder_inputs)\n",
    "\n",
    "        # Initialize decoder input (<START> token)\n",
    "        decoder_hidden, decoder_cell = encoder_hidden, encoder_cell\n",
    "        # decoder_input = tf.expand_dims(decoder_inputs, 1)\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        for t in range(1, decoder_targets.shape[1]):\n",
    "            predictions, decoder_hidden, decoder_cell = decoder(decoder_inputs, encoder_outputs, decoder_hidden, decoder_cell)\n",
    "\n",
    "            loss = loss_object(decoder_targets[:, t], predictions)\n",
    "            total_loss += tf.reduce_mean(loss)\n",
    "\n",
    "            # Teacher forcing: 50% probability of using true target as next input\n",
    "            if np.random.rand() < 0.5:\n",
    "                decoder_input = tf.expand_dims(decoder_targets[:, t], 1)\n",
    "            else:\n",
    "                decoder_input = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "    gradients = tape.gradient(total_loss, encoder.trainable_variables + decoder.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, encoder.trainable_variables + decoder.trainable_variables))\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for batch in range(0, len(all_inputs), BATCH_SIZE):\n",
    "        batch_inputs = all_input_emb[batch : batch + BATCH_SIZE]\n",
    "        batch_targets = all_target_emb[batch : batch + BATCH_SIZE]\n",
    "        decode_target = target_seqs[batch : batch + BATCH_SIZE]\n",
    "\n",
    "        # 🛠 Apply padding\n",
    "        batch_inputs = pad_embeddings(batch_inputs, MAX_SEQ_LENGTH, embedding_dim=150)\n",
    "        batch_targets = pad_embeddings(batch_targets, MAX_SEQ_LENGTH, embedding_dim=150)\n",
    "\n",
    "        # Convert to TensorFlow tensors\n",
    "        batch_inputs = tf.convert_to_tensor(batch_inputs, dtype=tf.float16)\n",
    "        batch_targets = tf.convert_to_tensor(batch_targets, dtype=tf.float16)\n",
    "\n",
    "        # Train step\n",
    "        batch_loss = train_step(batch_inputs, batch_targets[:, :-1], decode_target[:, 1:])\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(all_inputs)}\")\n",
    "\n",
    "print(\"✅ Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 244)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (300,) into shape (150,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 59\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embedding_matrix\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Prepare embedding matrices\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m input_embedding_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mget_embedding_matrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mall_input_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_target_emb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m target_embedding_matrix \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     63\u001b[0m     input_embedding_matrix  \u001b[38;5;66;03m# Reuse for simplicity; separate if needed\u001b[39;00m\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Encoder Class\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 54\u001b[0m, in \u001b[0;36mget_embedding_matrix\u001b[0;34m(all_emb_list, tokenizer, embedding_dim)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m emb_list:\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;66;03m# This assumes emb_list aligns with tokenized words; adjust as needed\u001b[39;00m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, vec \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(emb):\n\u001b[0;32m---> 54\u001b[0m             \u001b[43membedding_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m vec  \u001b[38;5;66;03m# +1 to skip padding index 0\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embedding_matrix\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (300,) into shape (150,)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Bidirectional,\n",
    "    Attention,\n",
    "    Concatenate,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# Assuming your data preparation (from earlier)\n",
    "all_inputs, all_targets, all_input_emb, all_target_emb = prepare_zst_data(\n",
    "    eng_span, eng_port\n",
    ")\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(all_inputs + all_targets)\n",
    "input_seqs = tokenizer.texts_to_sequences(all_inputs)\n",
    "target_seqs = tokenizer.texts_to_sequences(all_targets)\n",
    "max_len = max(max(len(seq) for seq in input_seqs), max(len(seq) for seq in target_seqs))\n",
    "input_seqs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    input_seqs, maxlen=max_len, padding=\"post\"\n",
    ")\n",
    "target_seqs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    target_seqs, maxlen=max_len, padding=\"post\"\n",
    ")\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "embedding_dim = 150  # Your reduced embedding dimension\n",
    "hidden_units = 128  # As in the paper\n",
    "vocab_size = 10000\n",
    "\n",
    "\n",
    "# Create embedding lookup function (map token IDs to embeddings)\n",
    "def get_embedding_matrix(all_emb_list, tokenizer, embedding_dim):\n",
    "    vocab_size = len(tokenizer.word_index) + 1  # +1 for padding (0)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    word_to_idx = {\n",
    "        word: idx for word, idx in tokenizer.word_index.items() if idx < vocab_size\n",
    "    }\n",
    "\n",
    "    # Flatten all embeddings and map to words (simplified; assumes alignment)\n",
    "    for emb_list in all_emb_list:  # e.g., all_input_emb or all_target_emb\n",
    "        for emb in emb_list:\n",
    "            # This assumes emb_list aligns with tokenized words; adjust as needed\n",
    "            for i, vec in enumerate(emb):\n",
    "                embedding_matrix[i + 1] = vec  # +1 to skip padding index 0\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "# Prepare embedding matrices\n",
    "input_embedding_matrix = get_embedding_matrix(\n",
    "    [all_input_emb, all_target_emb], tokenizer, embedding_dim\n",
    ")\n",
    "target_embedding_matrix = (\n",
    "    input_embedding_matrix  # Reuse for simplicity; separate if needed\n",
    ")\n",
    "\n",
    "\n",
    "# Encoder Class\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim=150, hidden_units=128):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.lstm = Bidirectional(\n",
    "            LSTM(hidden_units, return_sequences=True, return_state=True)\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoder_outputs, forward_h, forward_c, backward_h, backward_c = self.lstm(\n",
    "            inputs\n",
    "        )\n",
    "        encoder_hidden = tf.concat(\n",
    "            [forward_h, backward_h], axis=-1\n",
    "        )  # (batch_size, hidden_units * 2)\n",
    "        encoder_cell = tf.concat(\n",
    "            [forward_c, backward_c], axis=-1\n",
    "        )  # (batch_size, hidden_units * 2)\n",
    "        return encoder_outputs, encoder_hidden, encoder_cell\n",
    "\n",
    "\n",
    "# Decoder Class\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim=150, hidden_units=128, vocab_size=10000):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_units = hidden_units * 2  # Match encoder's bidirectional output\n",
    "        self.lstm = LSTM(self.hidden_units, return_sequences=True, return_state=True)\n",
    "        self.attention = Attention()\n",
    "        self.dense = Dense(vocab_size, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, hidden_state, cell_state):\n",
    "        lstm_output, new_hidden, new_cell = self.lstm(\n",
    "            inputs, initial_state=[hidden_state, cell_state]\n",
    "        )\n",
    "        context_vector = self.attention([lstm_output, encoder_outputs])\n",
    "        concat_output = tf.concat([lstm_output, context_vector], axis=-1)\n",
    "        output = self.dense(concat_output)\n",
    "        return output, new_hidden, new_cell\n",
    "\n",
    "\n",
    "# Instantiate models\n",
    "encoder = Encoder(embedding_dim=embedding_dim, hidden_units=hidden_units)\n",
    "decoder = Decoder(\n",
    "    embedding_dim=embedding_dim, hidden_units=hidden_units, vocab_size=vocab_size\n",
    ")\n",
    "\n",
    "# Optimizer and Loss\n",
    "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "# Training Step\n",
    "@tf.function\n",
    "def train_step(\n",
    "    encoder_inputs, decoder_inputs, decoder_targets, input_emb_matrix, target_emb_matrix\n",
    "):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Embed encoder inputs (token IDs -> embeddings)\n",
    "        encoder_inputs_embedded = tf.nn.embedding_lookup(\n",
    "            input_emb_matrix, encoder_inputs\n",
    "        )  # (batch_size, max_len, 150)\n",
    "\n",
    "        # Encode\n",
    "        encoder_outputs, encoder_hidden, encoder_cell = encoder(encoder_inputs_embedded)\n",
    "\n",
    "        # Initialize decoder\n",
    "        decoder_hidden, decoder_cell = encoder_hidden, encoder_cell\n",
    "        total_loss = 0\n",
    "\n",
    "        # Time steps for teacher forcing\n",
    "        for t in range(decoder_inputs.shape[1]):\n",
    "            # Embed decoder input for current time step\n",
    "            current_input = decoder_inputs[:, t]  # (batch_size,)\n",
    "            decoder_input_embedded = tf.nn.embedding_lookup(\n",
    "                target_emb_matrix, current_input\n",
    "            )  # (batch_size, 150)\n",
    "            decoder_input_embedded = tf.expand_dims(\n",
    "                decoder_input_embedded, 1\n",
    "            )  # (batch_size, 1, 150)\n",
    "\n",
    "            # Decode\n",
    "            predictions, decoder_hidden, decoder_cell = decoder(\n",
    "                decoder_input_embedded, encoder_outputs, decoder_hidden, decoder_cell\n",
    "            )\n",
    "\n",
    "            # Loss\n",
    "            loss = loss_object(\n",
    "                decoder_targets[:, t], predictions[:, 0, :]\n",
    "            )  # Remove time dim from predictions\n",
    "            total_loss += tf.reduce_mean(loss)\n",
    "\n",
    "            # Teacher forcing: 50% chance of using true target, 50% predicted\n",
    "            if np.random.rand() < 0.5:\n",
    "                decoder_input_next = decoder_targets[:, t]\n",
    "            else:\n",
    "                decoder_input_next = tf.argmax(predictions[:, 0, :], axis=-1)\n",
    "            decoder_inputs = tf.tensor_scatter_nd_update(\n",
    "                decoder_inputs, [[i, t] for i in range(BATCH_SIZE)], decoder_input_next\n",
    "            )\n",
    "\n",
    "    gradients = tape.gradient(\n",
    "        total_loss, encoder.trainable_variables + decoder.trainable_variables\n",
    "    )\n",
    "    optimizer.apply_gradients(\n",
    "        zip(gradients, encoder.trainable_variables + decoder.trainable_variables)\n",
    "    )\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for batch in range(0, len(input_seqs), BATCH_SIZE):\n",
    "        batch_inputs = input_seqs[batch : batch + BATCH_SIZE]\n",
    "        batch_decoder_inputs = target_seqs[batch : batch + BATCH_SIZE, :-1]\n",
    "        batch_decoder_targets = target_seqs[batch : batch + BATCH_SIZE, 1:]\n",
    "\n",
    "        batch_loss = train_step(\n",
    "            batch_inputs,\n",
    "            batch_decoder_inputs,\n",
    "            batch_decoder_targets,\n",
    "            input_embedding_matrix,\n",
    "            target_embedding_matrix,\n",
    "        )\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / (len(input_seqs) // BATCH_SIZE)}\")\n",
    "\n",
    "print(\"✅ Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def translate(sentence_embedding):\n",
    "    encoder_outputs, encoder_hidden, encoder_cell = encoder(tf.expand_dims(sentence_embedding, 0))\n",
    "\n",
    "    decoder_hidden, decoder_cell = encoder_hidden, encoder_cell\n",
    "    decoder_input = tf.expand_dims([start_token], 0)  # Start with `<START>`\n",
    "\n",
    "    translated_sentence = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        predictions, decoder_hidden, decoder_cell = decoder(decoder_input, encoder_outputs, decoder_hidden, decoder_cell)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions, axis=-1).numpy()[0, 0]\n",
    "        if predicted_id == end_token:\n",
    "            break\n",
    "\n",
    "        translated_sentence.append(tokenizer.index_word[predicted_id])\n",
    "        decoder_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return \" \".join(translated_sentence)\n",
    "\n",
    "# BLEU Score Evaluation\n",
    "def evaluate_bleu(test_inputs, test_targets):\n",
    "    bleu_scores = []\n",
    "\n",
    "    for i in range(len(test_inputs)):\n",
    "        reference = test_targets[i].split()\n",
    "        candidate = translate(test_inputs[i]).split()\n",
    "        bleu = sentence_bleu([reference], candidate)\n",
    "        bleu_scores.append(bleu)\n",
    "\n",
    "    return np.mean(bleu_scores)\n",
    "\n",
    "# Example Test\n",
    "test_bleu = evaluate_bleu(test_data, test_target)\n",
    "print(f\"BLEU Score: {test_bleu:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
